{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLOp3W1o5ENS",
        "outputId": "3daf093c-cca0-41b1-f580-373b2db1b2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 74 (delta 29), reused 61 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 8.98 MiB | 26.14 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/transformers/few_shot_learning\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/cervs257/transformers\n",
        "# %cd transformers/few_shot_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "id": "Ba9jIIhI49Kr"
      },
      "outputs": [],
      "source": [
        "from scratch_transformer import MultiHeadAttentionBlock\n",
        "from data import create_weights, get_reg_data, get_nonlinear_data\n",
        "import numpy as np\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-3\n",
        "\n",
        "# linear attention params override\n",
        "la_params = create_weights(feature_size, output_size, N, lr)\n",
        "\n",
        "# get the data\n",
        "eval_data = get_reg_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JIeEmuz949Ks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Now we will override the weights of the model to implement those that perform GD in the forward pass\n",
        "def override_weights(model, new_params, w_name):\n",
        "    w_name = \"Transformer_gd/multi_head_attention/\" + w_name\n",
        "    w_numpy = new_params[w_name][\"w\"]\n",
        "    w_tensor = torch.tensor(w_numpy, dtype=model.weight.dtype)\n",
        "    model.weight.data = w_tensor\n",
        "\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha.w_q, la_params, \"query\")\n",
        "override_weights(mha.w_k, la_params, \"key\")\n",
        "override_weights(mha.w_v, la_params, \"value\")\n",
        "override_weights(mha.w_o, la_params, \"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "JnmTyGrv49Ks"
      },
      "outputs": [],
      "source": [
        "def compute_loss(preds, targets):\n",
        "    \"\"\"Compute the MSE loss.\"\"\"\n",
        "    return 0.5 * np.sum((targets - preds) ** 2) / targets.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0fnTyjvR49Ks"
      },
      "outputs": [],
      "source": [
        "e_eval = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "# Forward pass\n",
        "out = mha(e_eval, e_eval, e_eval)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_targets = eval_data[1][:, -1]\n",
        "eval_preds = out[:, -1, -1] * (-1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC5hxQxF49Ks",
        "outputId": "5c5d898b-b031-4536-9f8f-f30e37352abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for M: 10, N: 1000 is 0.484.\n"
          ]
        }
      ],
      "source": [
        "loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "print(f\"Loss for M: {M}, N: {N} is {loss:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6xxrFe7R49Kt"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=1000,\n",
        "    linear_data=False,\n",
        "    model_type=\"attn\",\n",
        "    mask=None,\n",
        "    stocks_train=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    param model_type: str, \"attn\" or \"transformer\"\n",
        "    \"\"\"\n",
        "    assert model_type in [\n",
        "        \"attn\",\n",
        "        \"transformer\",\n",
        "    ], \"model_type must be 'attn' or 'transformer'\"\n",
        "    if stocks_train is not None:\n",
        "        assert eval_data is not None, \"No stock evaluation data provided.\"\n",
        "    eval_losses = []\n",
        "    lowest_loss = 1e9\n",
        "\n",
        "    # Move the model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Training on {device}.\")\n",
        "\n",
        "    # If using stock data, we're predicting 5 outomes\n",
        "    if stocks_train is not None:\n",
        "        no_outcomes = 5\n",
        "    else:\n",
        "        no_outcomes = 1\n",
        "\n",
        "    # Get the evaluation data if it is not provided\n",
        "    if eval_data is None:\n",
        "        if linear_data:\n",
        "            eval_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            eval_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "    assert eval_data is not None, \"No evaluation data provided.\"\n",
        "    e_eval = torch.tensor(eval_data[0]).float().to(device)\n",
        "    eval_targets = (\n",
        "        torch.tensor(eval_data[1][:, -no_outcomes:]).float().to(device)\n",
        "    )  # change for stocks\n",
        "\n",
        "    # Define lr scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1000, gamma=0.5)\n",
        "\n",
        "    for step in tqdm(range(training_steps + 1)):\n",
        "        # Generate train data\n",
        "        if stocks_train is not None:\n",
        "            train_data = stocks_train\n",
        "        elif linear_data:\n",
        "            train_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            train_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        e_train = torch.tensor(train_data[0]).float().to(device)\n",
        "        targets = (\n",
        "            torch.tensor(train_data[1][:, -no_outcomes:]).float().to(device)\n",
        "        )  # change for stocks\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        if model_type == \"attn\":\n",
        "            out = model(e_train, e_train, e_train, mask)\n",
        "        else:\n",
        "            out = model(e_train, mask)\n",
        "        preds = out[:, -1, -no_outcomes:] * (-1.0)  # change for stocks\n",
        "        loss = criterion(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate\n",
        "        if step % 100 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                if model_type == \"attn\":\n",
        "                    ev_preds = model(e_eval, e_eval, e_eval)\n",
        "                else:\n",
        "                    ev_preds = model(e_eval, None)  # no mask in evaluation mode\n",
        "                ev_preds = ev_preds[:, -1, -no_outcomes:] * (-1.0)  # change for stocks\n",
        "                eval_loss = criterion(ev_preds, eval_targets)\n",
        "                eval_losses.append(eval_loss)\n",
        "            model.train()\n",
        "            if eval_loss < lowest_loss:\n",
        "                lowest_loss = eval_loss\n",
        "                if stocks_train is not None:\n",
        "                    data_type = \"stocks\"\n",
        "                elif linear_data:\n",
        "                    data_type = \"lin_data\"\n",
        "                else:\n",
        "                    data_type = \"nonlin_data\"\n",
        "                if model_type == \"transformer\":\n",
        "                    att = \"transformer\"\n",
        "                elif model.softmax_att:\n",
        "                    att = \"softmax_attn\"\n",
        "                else:\n",
        "                    att = \"linear_attn\"\n",
        "                path = f\"models/{att}-{data_type}.pth\"\n",
        "                torch.save(model.state_dict(), path)\n",
        "            print(f\"Step {step}, Train Loss: {loss.item():.3f}\")\n",
        "            print(f\"Step {step}, Eval Loss: {eval_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEqqs2G49Kt",
        "outputId": "977e0c6f-f9f8-4e35-aff5-039a00562fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1001 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'get_reg_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m      8\u001b[0m training_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlinear_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[17], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, criterion, eval_data, training_steps, linear_data, model_type, mask, stocks_train)\u001b[0m\n\u001b[0;32m     61\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m stocks_train\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m linear_data:\n\u001b[1;32m---> 63\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_reg_data\u001b[49m(\n\u001b[0;32m     64\u001b[0m         no_tasks\u001b[38;5;241m=\u001b[39mM, feature_size\u001b[38;5;241m=\u001b[39mfeature_size, no_examples\u001b[38;5;241m=\u001b[39mN\n\u001b[0;32m     65\u001b[0m     )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m get_nonlinear_data(\n\u001b[0;32m     68\u001b[0m         no_tasks\u001b[38;5;241m=\u001b[39mM, feature_size\u001b[38;5;241m=\u001b[39mfeature_size, no_examples\u001b[38;5;241m=\u001b[39mN\n\u001b[0;32m     69\u001b[0m     )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'get_reg_data' is not defined"
          ]
        }
      ],
      "source": [
        "# Now let's explore training the model\n",
        "import torch.optim as optim\n",
        "\n",
        "# Train\n",
        "optimizer = optim.Adam(mha.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "train(\n",
        "    mha,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=True,\n",
        "    model_type=\"attn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ity6FBVo49Kt",
        "outputId": "ad566c23-bf13-4eb0-b022-018aabddb0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss pre override for M: 10, N: 1000 is 1059.016.\n",
            "Loss with GD weights for M: 10, N: 1000 is 0.603.\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "# Let's do the same but with non linear data\n",
        "eval_nl_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e_eval_nl = torch.tensor(eval_nl_data[0]).float()\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Forward pass pre override\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss pre override for M: {M}, N: {N} is {loss_nl:.3f}.\")\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha_nl.w_q, la_params, \"query\")\n",
        "override_weights(mha_nl.w_k, la_params, \"key\")\n",
        "override_weights(mha_nl.w_v, la_params, \"value\")\n",
        "override_weights(mha_nl.w_o, la_params, \"linear\")\n",
        "\n",
        "# Forward pass\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss with GD weights for M: {M}, N: {N} is {loss_nl:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CSkn2F49Kt",
        "outputId": "86db883d-4d89-41c1-bde0-726f5592028b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 1.222\n",
            "Step 0, Eval Loss: 8.459\n",
            "Step 100, Train Loss: 1.940\n",
            "Step 100, Eval Loss: 2.947\n",
            "Step 200, Train Loss: 1.526\n",
            "Step 200, Eval Loss: 1.124\n",
            "Step 300, Train Loss: 2.347\n",
            "Step 300, Eval Loss: 1.023\n",
            "Step 400, Train Loss: 1.603\n",
            "Step 400, Eval Loss: 1.079\n",
            "Step 500, Train Loss: 1.361\n",
            "Step 500, Eval Loss: 1.208\n",
            "Step 600, Train Loss: 2.761\n",
            "Step 600, Eval Loss: 1.065\n",
            "Step 700, Train Loss: 0.738\n",
            "Step 700, Eval Loss: 1.183\n",
            "Step 800, Train Loss: 1.057\n",
            "Step 800, Eval Loss: 1.669\n",
            "Step 900, Train Loss: 2.104\n",
            "Step 900, Eval Loss: 1.173\n",
            "Step 1000, Train Loss: 1.093\n",
            "Step 1000, Eval Loss: 1.194\n",
            "Step 1100, Train Loss: 0.666\n",
            "Step 1100, Eval Loss: 1.440\n",
            "Step 1200, Train Loss: 1.182\n",
            "Step 1200, Eval Loss: 1.303\n",
            "Step 1300, Train Loss: 1.037\n",
            "Step 1300, Eval Loss: 1.566\n",
            "Step 1400, Train Loss: 1.827\n",
            "Step 1400, Eval Loss: 1.525\n",
            "Step 1500, Train Loss: 0.792\n",
            "Step 1500, Eval Loss: 1.274\n",
            "Step 1600, Train Loss: 1.173\n",
            "Step 1600, Eval Loss: 1.125\n",
            "Step 1700, Train Loss: 1.082\n",
            "Step 1700, Eval Loss: 1.044\n",
            "Step 1800, Train Loss: 0.858\n",
            "Step 1800, Eval Loss: 1.007\n",
            "Step 1900, Train Loss: 0.447\n",
            "Step 1900, Eval Loss: 0.985\n",
            "Step 2000, Train Loss: 1.200\n",
            "Step 2000, Eval Loss: 1.018\n",
            "Step 2100, Train Loss: 1.545\n",
            "Step 2100, Eval Loss: 1.113\n",
            "Step 2200, Train Loss: 1.016\n",
            "Step 2200, Eval Loss: 1.070\n",
            "Step 2300, Train Loss: 0.966\n",
            "Step 2300, Eval Loss: 1.090\n",
            "Step 2400, Train Loss: 0.610\n",
            "Step 2400, Eval Loss: 1.077\n",
            "Step 2500, Train Loss: 0.830\n",
            "Step 2500, Eval Loss: 0.959\n",
            "Step 2600, Train Loss: 0.696\n",
            "Step 2600, Eval Loss: 0.933\n",
            "Step 2700, Train Loss: 0.993\n",
            "Step 2700, Eval Loss: 1.157\n",
            "Step 2800, Train Loss: 0.278\n",
            "Step 2800, Eval Loss: 1.183\n",
            "Step 2900, Train Loss: 0.645\n",
            "Step 2900, Eval Loss: 0.987\n",
            "Step 3000, Train Loss: 1.080\n",
            "Step 3000, Eval Loss: 1.197\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "optimizer = optim.Adam(mha_nl.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 3000\n",
        "\n",
        "# Now let's explore training the model\n",
        "train(\n",
        "    mha_nl,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_nl_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqMN7ZO049Kt",
        "outputId": "c20dadb0-ed18-46cc-af6c-dc74d32f8e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 0.778\n",
            "Step 0, Eval Loss: 0.747\n",
            "Step 100, Train Loss: 0.854\n",
            "Step 100, Eval Loss: 0.677\n",
            "Step 200, Train Loss: 0.770\n",
            "Step 200, Eval Loss: 0.567\n",
            "Step 300, Train Loss: 0.759\n",
            "Step 300, Eval Loss: 0.455\n",
            "Step 400, Train Loss: 0.794\n",
            "Step 400, Eval Loss: 0.432\n",
            "Step 500, Train Loss: 0.550\n",
            "Step 500, Eval Loss: 0.414\n",
            "Step 600, Train Loss: 0.581\n",
            "Step 600, Eval Loss: 0.449\n",
            "Step 700, Train Loss: 1.445\n",
            "Step 700, Eval Loss: 0.418\n",
            "Step 800, Train Loss: 0.475\n",
            "Step 800, Eval Loss: 0.403\n",
            "Step 900, Train Loss: 0.499\n",
            "Step 900, Eval Loss: 0.399\n",
            "Step 1000, Train Loss: 0.486\n",
            "Step 1000, Eval Loss: 0.427\n"
          ]
        }
      ],
      "source": [
        "# Finally let's use softmax attention\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl_sa = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "optimizer = optim.Adam(mha_nl_sa.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    mha_nl_sa,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scratch_transformer import (\n",
        "    LayerNormalization,\n",
        "    FeedForwardBlock,\n",
        "    ResidualConnection,\n",
        "    EncoderBlock,\n",
        "    MultiHeadAttentionBlock,\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from data import get_nonlinear_data\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-4\n",
        "dropout = 0.2\n",
        "mask = None\n",
        "\n",
        "# get the data\n",
        "eval_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "\n",
        "# MLP dimension usually 4 times the d_model\n",
        "# Residual connection already contains layer normalizations\n",
        "\n",
        "# Start with Self Attention\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=dropout, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "\n",
        "# Feed Forward\n",
        "ff = FeedForwardBlock(\n",
        "    d_model=feature_size + 1, d_ff=4 * (feature_size + 1), dropout=dropout\n",
        ")  # (batch_size, seq_len, d_model\n",
        "\n",
        "\n",
        "# Create an EncoderBlock\n",
        "eb = EncoderBlock(\n",
        "    self_attention_block=mha,\n",
        "    feed_forward_block=ff,\n",
        "    dropout=dropout,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu.\n",
            "Step 0, Train Loss: 37.812\n",
            "Step 0, Eval Loss: 5.308\n",
            "Step 100, Train Loss: 12.215\n",
            "Step 100, Eval Loss: 5.267\n",
            "Step 200, Train Loss: 7.803\n",
            "Step 200, Eval Loss: 5.223\n",
            "Step 300, Train Loss: 4.062\n",
            "Step 300, Eval Loss: 5.186\n",
            "Step 400, Train Loss: 2.614\n",
            "Step 400, Eval Loss: 5.146\n",
            "Step 500, Train Loss: 5.747\n",
            "Step 500, Eval Loss: 5.104\n",
            "Step 600, Train Loss: 24.885\n",
            "Step 600, Eval Loss: 5.065\n",
            "Step 700, Train Loss: 11.340\n",
            "Step 700, Eval Loss: 5.025\n",
            "Step 800, Train Loss: 25.919\n",
            "Step 800, Eval Loss: 4.989\n",
            "Step 900, Train Loss: 7.570\n",
            "Step 900, Eval Loss: 4.948\n",
            "Step 1000, Train Loss: 15.817\n",
            "Step 1000, Eval Loss: 4.906\n"
          ]
        }
      ],
      "source": [
        "training_steps = 1000\n",
        "optimizer = optim.Adam(eb.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in eb.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    eb,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        "    model_type=\"transformer\",\n",
        "    mask=mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>LAST</th>\n",
              "      <th>OPEN</th>\n",
              "      <th>LOW</th>\n",
              "      <th>HIGH</th>\n",
              "      <th>3M IMPLIED VOL</th>\n",
              "      <th>SHORT INTEREST RATIO</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>tweet_sentiment</th>\n",
              "      <th>news_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2006-02-06</td>\n",
              "      <td>126.60</td>\n",
              "      <td>126.44</td>\n",
              "      <td>126.17</td>\n",
              "      <td>126.80</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2006-02-07</td>\n",
              "      <td>125.48</td>\n",
              "      <td>126.30</td>\n",
              "      <td>125.40</td>\n",
              "      <td>126.66</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006-02-08</td>\n",
              "      <td>126.62</td>\n",
              "      <td>125.88</td>\n",
              "      <td>125.60</td>\n",
              "      <td>126.78</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2006-02-09</td>\n",
              "      <td>126.41</td>\n",
              "      <td>126.85</td>\n",
              "      <td>126.37</td>\n",
              "      <td>127.60</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2006-02-10</td>\n",
              "      <td>126.64</td>\n",
              "      <td>126.42</td>\n",
              "      <td>125.45</td>\n",
              "      <td>127.13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         DATE    LAST    OPEN     LOW    HIGH  3M IMPLIED VOL  \\\n",
              "0  2006-02-06  126.60  126.44  126.17  126.80             NaN   \n",
              "1  2006-02-07  125.48  126.30  125.40  126.66             NaN   \n",
              "2  2006-02-08  126.62  125.88  125.60  126.78             NaN   \n",
              "3  2006-02-09  126.41  126.85  126.37  127.60             NaN   \n",
              "4  2006-02-10  126.64  126.42  125.45  127.13             NaN   \n",
              "\n",
              "   SHORT INTEREST RATIO Ticker  tweet_sentiment  news_sentiment  \n",
              "0                   NaN    SPY              NaN             NaN  \n",
              "1                   NaN    SPY              NaN             NaN  \n",
              "2                   NaN    SPY              NaN             NaN  \n",
              "3                   NaN    SPY              NaN             NaN  \n",
              "4                   NaN    SPY              NaN             NaN  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "stocks = pd.read_csv(\"data/stocks.csv\")\n",
        "stocks.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "spy = stocks.loc[\n",
        "    stocks[\"Ticker\"] == \"SPY\", [\"DATE\", \"LAST\", \"OPEN\", \"LOW\", \"HIGH\", \"3M IMPLIED VOL\"]\n",
        "].dropna()\n",
        "spy.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Select all columns except 'DATE'\n",
        "cols = [col for col in spy.columns if col not in [\"DATE\", \"3M IMPLIED VOL\"]]\n",
        "\n",
        "# Apply the function to each element of the selected columns\n",
        "spy[cols] = spy[cols].apply(lambda x: np.log(x / 200))\n",
        "\n",
        "# Normalize the '3M IMPLIED VOL' column\n",
        "spy[\"3M IMPLIED VOL\"] = spy[\"3M IMPLIED VOL\"] / spy[\"3M IMPLIED VOL\"].std()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create y\n",
        "spy[\"price_1d\"] = spy[\"LAST\"].shift(-1)\n",
        "spy[\"price_5d\"] = spy[\"LAST\"].shift(-5)\n",
        "spy[\"price_10d\"] = spy[\"LAST\"].shift(-10)\n",
        "spy[\"price_20d\"] = spy[\"LAST\"].shift(-20)\n",
        "spy[\"open_1d\"] = spy[\"OPEN\"].shift(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# divide the data into periods of 251 days\n",
        "n = 251\n",
        "spy[\"period\"] = spy.index // n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\cerva\\AppData\\Local\\Temp\\ipykernel_50520\\640372639.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  spy.groupby(\"period\")\n"
          ]
        }
      ],
      "source": [
        "# for each period, separate the last 20 rows to avoid lookahead bias and use as eval data\n",
        "spy_val = spy.groupby(\"period\").tail(20)\n",
        "spy = (\n",
        "    spy.groupby(\"period\")\n",
        "    .apply(\n",
        "        lambda x: x.iloc[:-20], include_groups=True\n",
        "    )  # include groups to later create test query\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "spy_val = spy_val.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>LAST</th>\n",
              "      <th>OPEN</th>\n",
              "      <th>LOW</th>\n",
              "      <th>HIGH</th>\n",
              "      <th>3M IMPLIED VOL</th>\n",
              "      <th>price_1d</th>\n",
              "      <th>price_5d</th>\n",
              "      <th>price_10d</th>\n",
              "      <th>price_20d</th>\n",
              "      <th>open_1d</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>period</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-04-15</td>\n",
              "      <td>0.050836</td>\n",
              "      <td>0.049028</td>\n",
              "      <td>0.048552</td>\n",
              "      <td>0.053730</td>\n",
              "      <td>2.654142</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-04-13</td>\n",
              "      <td>0.039225</td>\n",
              "      <td>0.034401</td>\n",
              "      <td>0.033628</td>\n",
              "      <td>0.039701</td>\n",
              "      <td>2.837641</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-04-11</td>\n",
              "      <td>0.161523</td>\n",
              "      <td>0.160843</td>\n",
              "      <td>0.154179</td>\n",
              "      <td>0.162034</td>\n",
              "      <td>2.174546</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-11</td>\n",
              "      <td>0.276722</td>\n",
              "      <td>0.275622</td>\n",
              "      <td>0.275318</td>\n",
              "      <td>0.283825</td>\n",
              "      <td>3.048831</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-04-10</td>\n",
              "      <td>0.365650</td>\n",
              "      <td>0.363844</td>\n",
              "      <td>0.362244</td>\n",
              "      <td>0.365996</td>\n",
              "      <td>2.474409</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              DATE      LAST      OPEN       LOW      HIGH  3M IMPLIED VOL  \\\n",
              "period                                                                       \n",
              "0       2015-04-15  0.050836  0.049028  0.048552  0.053730        2.654142   \n",
              "1       2016-04-13  0.039225  0.034401  0.033628  0.039701        2.837641   \n",
              "2       2017-04-11  0.161523  0.160843  0.154179  0.162034        2.174546   \n",
              "3       2018-04-11  0.276722  0.275622  0.275318  0.283825        3.048831   \n",
              "4       2019-04-10  0.365650  0.363844  0.362244  0.365996        2.474409   \n",
              "\n",
              "        price_1d  price_5d  price_10d  price_20d  open_1d  \n",
              "period                                                     \n",
              "0            0.0       0.0        0.0        0.0      0.0  \n",
              "1            0.0       0.0        0.0        0.0      0.0  \n",
              "2            0.0       0.0        0.0        0.0      0.0  \n",
              "3            0.0       0.0        0.0        0.0      0.0  \n",
              "4            0.0       0.0        0.0        0.0      0.0  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now let's fill the last entry of each period with 0.0\n",
        "columns = [\"price_1d\", \"price_5d\", \"price_10d\", \"price_20d\", \"open_1d\"]\n",
        "last_entry = spy.groupby(\"period\").apply(\n",
        "    lambda x: x.last_valid_index(), include_groups=False\n",
        ")\n",
        "# spy_targets = np.array(spy.loc[last_entry, :].drop(columns=[\"DATE\", \"period\"]))\n",
        "spy_targets = np.array(spy.loc[last_entry, columns])\n",
        "spy.loc[last_entry, columns] = 0.0\n",
        "\n",
        "# Let's fill y data for spy_val with 0.0\n",
        "last_entry = spy_val.groupby(\"period\").apply(\n",
        "    lambda x: x.last_valid_index(), include_groups=False\n",
        ")\n",
        "spy_val_targets = np.array(spy_val.loc[last_entry, columns])\n",
        "spy_val.loc[last_entry, columns] = 0.0\n",
        "\n",
        "spy.groupby(\"period\").last().head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We're going to treat each period as a context\n",
        "spy = spy.drop(columns=[\"DATE\"])\n",
        "spy_val = spy_val.drop(columns=[\"DATE\"])\n",
        "groups = [group.drop(columns=[\"period\"]).values for _, group in spy.groupby(\"period\")]\n",
        "groups_val = [\n",
        "    group.drop(columns=[\"period\"]).values for _, group in spy_val.groupby(\"period\")\n",
        "]\n",
        "spy_np = np.array(groups)\n",
        "spy_val_np = np.array(groups_val)\n",
        "\n",
        "# Define stock eval data\n",
        "spy_eval = (spy_val_np, spy_val_targets)\n",
        "# Define stock train data\n",
        "spy_train = (spy_np, spy_targets)\n",
        "# eval_targets = torch.tensor(spy_eval[1]).float()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "mask = create_look_ahead_mask(spy_np.shape[1])\n",
        "\n",
        "\n",
        "# def create_look_ahead_mask(batch_size, seq_len):\n",
        "#     mask = torch.triu(torch.ones((batch_size, seq_len, seq_len)), diagonal=1)\n",
        "#     return mask  # (batch_size, seq_len, seq_len)\n",
        "\n",
        "# # Masks for the transformer\n",
        "# mask = create_look_ahead_mask(spy_np.shape[0], spy_np.shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scratch_transformer import (\n",
        "    LayerNormalization,\n",
        "    FeedForwardBlock,\n",
        "    ResidualConnection,\n",
        "    EncoderBlock,\n",
        "    MultiHeadAttentionBlock,\n",
        "    Encoder,\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "dropout = 0.2\n",
        "mask = mask\n",
        "heads = 5\n",
        "layers = 12\n",
        "\n",
        "# convert to tensor\n",
        "e = torch.tensor(spy_eval[0]).float()\n",
        "et = torch.tensor(spy_train[0]).float()\n",
        "\n",
        "\n",
        "# MLP dimension usually 4 times the d_model\n",
        "# Residual connection already contains layer normalizations\n",
        "\n",
        "# Start with Self Attention\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size,\n",
        "    heads=heads,\n",
        "    dropout=dropout,\n",
        "    softmax_att=True,\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# out = mha(e, e, e)\n",
        "out = mha(et, et, et, mask)\n",
        "\n",
        "# Feed Forward\n",
        "ff = FeedForwardBlock(\n",
        "    d_model=feature_size, d_ff=4 * feature_size, dropout=dropout\n",
        ")  # (batch_size, seq_len, d_model\n",
        "\n",
        "\n",
        "# Create an EncoderBlock\n",
        "eb = EncoderBlock(\n",
        "    self_attention_block=mha,\n",
        "    feed_forward_block=ff,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "encoder_blocks = []\n",
        "for _ in range(layers):\n",
        "    encoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "        d_model=feature_size, heads=heads, dropout=dropout, softmax_att=True\n",
        "    )\n",
        "    encoder_feed_forward_block = FeedForwardBlock(\n",
        "        d_model=feature_size, d_ff=4 * feature_size, dropout=dropout\n",
        "    )\n",
        "    encoder_block = EncoderBlock(\n",
        "        self_attention_block=encoder_self_attention_block,\n",
        "        feed_forward_block=encoder_feed_forward_block,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "# Don't worry about Encoder, it's just predefined in scratch_transformer\n",
        "decoder = Encoder(\n",
        "    nn.ModuleList(encoder_blocks),\n",
        ")\n",
        "\n",
        "# out = decoder(et, mask)\n",
        "\n",
        "for p in decoder.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Compare the output to the targets\n",
        "# eval_targets = spy_eval[1]\n",
        "# eval_preds = out[:, -1, -5:] * (-1.0)\n",
        "\n",
        "# loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "# print(f\"Loss is {loss:.3f}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 15530\n",
            "Training on cpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/1001 [00:00<07:17,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 0.604\n",
            "Step 0, Eval Loss: 0.282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 101/1001 [00:35<05:25,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 0.087\n",
            "Step 100, Eval Loss: 0.061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 201/1001 [01:09<04:35,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 0.077\n",
            "Step 200, Eval Loss: 0.053\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 301/1001 [01:44<04:01,  2.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 0.026\n",
            "Step 300, Eval Loss: 0.010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 401/1001 [02:19<03:26,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 0.018\n",
            "Step 400, Eval Loss: 0.008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 501/1001 [02:53<02:53,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 0.014\n",
            "Step 500, Eval Loss: 0.008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 601/1001 [03:27<02:19,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 0.014\n",
            "Step 600, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 701/1001 [04:02<01:43,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 0.014\n",
            "Step 700, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 801/1001 [04:45<01:25,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 0.009\n",
            "Step 800, Eval Loss: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 901/1001 [05:22<00:38,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 0.006\n",
            "Step 900, Eval Loss: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [05:59<00:00,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 0.013\n",
            "Step 1000, Eval Loss: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Train\n",
        "lr = 1e-3\n",
        "training_steps = 1000\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in decoder.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    decoder,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=spy_eval,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        "    model_type=\"transformer\",\n",
        "    mask=mask,\n",
        "    stocks_train=spy_train,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Loss: 0.004\n"
          ]
        }
      ],
      "source": [
        "# load the transformer-stocks.pth model\n",
        "decoder.load_state_dict(torch.load(\"models/transformer-stocks.pth\"))\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Evaluate the model\n",
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    e_eval = torch.tensor(spy_eval[0]).float()\n",
        "    eval_preds = decoder(e_eval, None)[:, -1, -5:] * (-1.0)\n",
        "    eval_targets = torch.tensor(spy_eval[1]).float()\n",
        "    eval_loss = criterion(eval_preds, eval_targets)\n",
        "\n",
        "print(f\"Eval Loss: {eval_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[224.5926, 220.6326, 227.9468, 223.9325, 221.0671],\n",
              "        [224.0863, 220.0589, 227.5154, 224.0515, 219.7355],\n",
              "        [242.0759, 240.4400, 241.9553, 237.2371, 241.6922],\n",
              "        [271.1071, 261.0004, 259.0979, 260.3919, 259.6585],\n",
              "        [288.6564, 282.7476, 274.6498, 274.1552, 289.5774],\n",
              "        [290.2488, 304.7810, 283.4862, 304.2762, 295.5924],\n",
              "        [468.8124, 454.3061, 468.3952, 461.0709, 461.0333],\n",
              "        [439.3058, 432.7989, 441.1549, 438.1042, 433.3932],\n",
              "        [454.2195, 442.6006, 455.5841, 449.9601, 446.8123],\n",
              "        [262.3158, 254.4384, 253.8929, 254.0308, 253.7223],\n",
              "        [291.6357, 284.6928, 276.4556, 277.0117, 289.1633],\n",
              "        [274.5508, 268.7983, 264.2328, 276.4927, 260.3329],\n",
              "        [441.7336, 433.7585, 444.6514, 441.3233, 433.8806],\n",
              "        [438.2299, 436.6476, 442.7344, 441.9445, 434.5273],\n",
              "        [442.7309, 431.7016, 444.6029, 441.0671, 433.7714]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.exp(eval_preds) * 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[212.2100, 212.8800, 212.4600, 211.6300, 211.2400],\n",
              "        [206.5600, 204.9100, 209.2800, 212.0800, 207.2900],\n",
              "        [239.3800, 235.8200, 240.6100, 243.7800, 239.3500],\n",
              "        [272.0200, 272.2400, 273.3600, 277.3700, 270.3400],\n",
              "        [288.1000, 287.7000, 282.1400, 287.6500, 285.6200],\n",
              "        [292.4400, 284.9700, 294.8800, 319.3400, 291.0900],\n",
              "        [422.1200, 410.2800, 415.2800, 422.6000, 419.8900],\n",
              "        [413.8100, 392.7500, 391.8600, 417.3900, 424.5500],\n",
              "        [412.6300, 412.1300, 419.2300, 427.9200, 408.9100],\n",
              "        [265.6400, 263.7600, 270.3900, 263.2000, 265.5500],\n",
              "        [287.1800, 288.2900, 289.4500, 291.1800, 286.7800],\n",
              "        [251.8300, 274.0300, 279.1000, 290.4800, 245.1900],\n",
              "        [400.6100, 408.5200, 415.8700, 420.0600, 398.4000],\n",
              "        [458.7000, 451.0300, 438.2900, 417.2700, 460.3400],\n",
              "        [403.7000, 407.6000, 413.4700, 412.4100, 404.0900]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.exp(eval_targets) * 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 0.013\n"
          ]
        }
      ],
      "source": [
        "# compare to XGBoost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Create the model\n",
        "model = XGBRegressor(\n",
        "    n_estimators=10000,\n",
        "    max_depth=100,\n",
        "    learning_rate=0.01,\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "train = spy_train[0].reshape(-1, 10)\n",
        "test = spy_eval[0].reshape(-1, 10)\n",
        "\n",
        "x_train = train[:, :5]\n",
        "y_train = train[:, 5:]\n",
        "\n",
        "x_test = test[:, :5]\n",
        "y_test = test[:, 5:]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Calculate the MSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"MSE: {mse:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
