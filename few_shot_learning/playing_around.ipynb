{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformers for Stock Price Prediction\n",
        "\n",
        "### ECE590 Final Project\n",
        "Name: Javier Cervantes\n",
        "\n",
        "net id: jc1010"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract\n",
        "\n",
        "The motivation behind this experiment is to evaluate whether Transformers are more capable of predicting stock prices than traditional machine learning models. Given the Transformer's capabilities to utilize and derive context from the input, this experiment will consist in utilizing the simplest possible inputs: historical price data. This shall lay the foundations for future experiments that can incorporate sentiment, fundamental and technical analysis data.\n",
        "\n",
        "After demonstrating that Transformers can perform gradient descent in their forward pass on linear and then on non-linear data, we proceeded to add enough complexity that would allow us to tackle the stock prediction problem. We compared the performance of the Transformer model with popular, powerful machine learning models such as the LSTM and XGBoost. The results showed that the Transformer model was able to outperform the LSTM and XGBoost models in predicting future stock prices.\n",
        "\n",
        "Note that this paper isn't intended to provide a trading strategy or a state of the art prediction tool, but rather to demonstrate the capabilities of Transformers in predicting stock prices compared to other traditional models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "To build up to our experiment, we shall first demonstrate how Transformers are able to perform gradient descent in their forward pass using linear self-attention on linear data. Upon successful completion of this task, we shall then add complexity to our model by introducing non linear self-attention mechanisms and evaluating on non-linear data. If this proves successful, we shall escalate the complexity of our model by introducing a MLP on top of the self-attention mechanism and evaluating on stock data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "• For the linear attention examples, we attempted to replicate the work done on this paper: https://arxiv.org/abs/2212.07677\n",
        "\n",
        "• For the examples with softmax attention, we followed this paper: https://arxiv.org/abs/2208.01066\n",
        "\n",
        "• Special thanks to `hkproj` for their lecture on building a Transformer from scratch: [Lecture](https://www.youtube.com/watch?v=ISNdQcPhsts) & [Repo](https://github.com/hkproj/pytorch-transformer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Self Attention\n",
        "\n",
        "In this section, we shall demonstrate how a Transformer can perform gradient descent in its forward pass using linear self-attention on linear data. \n",
        "\n",
        "### Data\n",
        "\n",
        "We implemented the following process for generating contextual data for a linear model: weights $w_m \\in \\mathbb{R}^{10}$ are drawn for context $m$ as $w_m \\sim \\mathcal{N}(\\mathcal{0}_d, I_{10})$ where $\\mu \\in \\mathbb{R}^{10}$ is a fixed mean vector. Covariates $x_i \\in \\mathbb{R}^{10}$ are drawn as $x_i \\sim \\mathcal{U}(-1, 1)$. For contextual data $\\mathcal{C}_m$ draw one weight vector $w_m$ as above. For a context of length $N$ draw $x_{m,i}, i = 1, \\ldots, N$ as above, and for each $x_{m, i}$ constitute a corresponding $y_{m, i} = w_m^T x_{m, i}$. \n",
        "\n",
        "The contextual data so drawn are represented as $\\mathcal{C}_m = (x_{m, 1}, y_{m, 1}, \\ldots, x_{m, N}, y_{m, N})$. Finally, we draw a query associated with $\\mathcal{C}_m, x_{m, N+1}$, and the goal is to predict $y_{m, N+1} $ given $x_{m, N+1}$ and $\\mathcal{C}_m$. \n",
        "\n",
        "### Weight Initialization\n",
        "\n",
        "Following the paper, we shall initialize the attention heads as follows:\n",
        "- $W_Q = W_K = \\begin{pmatrix} I_d & 0 \\\\ 0 & 0 \\end{pmatrix}$\n",
        "- $W_V = \\begin{pmatrix} \\mathcal{0}_{d \\times d} & \\mathcal{0}_{d \\times 1} \\\\ \\mathcal{0}_{1 \\times d} & 1 \\end{pmatrix}$\n",
        "- $P = \\begin{pmatrix} \\mathcal{0}_{d \\times d} & \\mathcal{0}_{d \\times 1} \\\\ \\mathcal{0}_{1 \\times d} & -\\alpha \\end{pmatrix}$\n",
        "\n",
        "Where $\\alpha$ is the learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nOjU8_6MkBEL"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ba9jIIhI49Kr"
      },
      "outputs": [],
      "source": [
        "from scratch_transformer import MultiHeadAttentionBlock\n",
        "from data import create_weights, get_reg_data, get_nonlinear_data\n",
        "import numpy as np\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-3\n",
        "\n",
        "# linear attention params override\n",
        "la_params = create_weights(feature_size, output_size, N, lr)\n",
        "\n",
        "# get the data\n",
        "eval_data = get_reg_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(preds, targets):\n",
        "    \"\"\"Compute the MSE loss.\"\"\"\n",
        "    return 0.5 * np.sum((targets - preds) ** 2) / targets.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forward Pass\n",
        "\n",
        "The forward pass of the Transformer model is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "e_eval = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "# Forward pass\n",
        "out = mha(e_eval, e_eval, e_eval)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_targets = eval_data[1][:, -1]\n",
        "eval_preds = out[:, -1, -1] * (-1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss pre weight override for M: 10, N: 1000 is 15.427.\n"
          ]
        }
      ],
      "source": [
        "loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "print(f\"Loss pre weight override for M: {M}, N: {N} is {loss:.3f}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JIeEmuz949Ks"
      },
      "outputs": [],
      "source": [
        "# Now we will override the weights of the model to implement those that perform GD in the forward pass\n",
        "def override_weights(model, new_params, w_name):\n",
        "    w_name = \"Transformer_gd/multi_head_attention/\" + w_name\n",
        "    w_numpy = new_params[w_name][\"w\"]\n",
        "    w_tensor = torch.tensor(w_numpy, dtype=model.weight.dtype)\n",
        "    model.weight.data = w_tensor\n",
        "\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha.w_q, la_params, \"query\")\n",
        "override_weights(mha.w_k, la_params, \"key\")\n",
        "override_weights(mha.w_v, la_params, \"value\")\n",
        "override_weights(mha.w_o, la_params, \"linear\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0fnTyjvR49Ks"
      },
      "outputs": [],
      "source": [
        "e_eval = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "# Forward pass\n",
        "out = mha(e_eval, e_eval, e_eval)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_targets = eval_data[1][:, -1]\n",
        "eval_preds = out[:, -1, -1] * (-1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC5hxQxF49Ks",
        "outputId": "075320df-c2e5-4ddd-af6d-847616df6571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for M: 10, N: 1000 is 0.350.\n"
          ]
        }
      ],
      "source": [
        "loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "print(f\"Loss for M: {M}, N: {N} is {loss:.3f}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we performed two forward passes on our Self Attention layer:\n",
        "1. With random initialization of the weights. This resulted in a very high loss. \n",
        "2. With the weights initialized as designed to perform GD on the forward pass. The loss decreased substantially to 0.35\n",
        "\n",
        "### Learning the parameters\n",
        "\n",
        "We shall proceed to perform a few steps of GD on the designed weights to see if we can achieve better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6xxrFe7R49Kt"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=1000,\n",
        "    linear_data=False,\n",
        "    model_type=\"attn\",\n",
        "    mask=None,\n",
        "    stocks_train=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    param model_type: str, \"attn\" or \"transformer\"\n",
        "    \"\"\"\n",
        "    assert model_type in [\n",
        "        \"attn\",\n",
        "        \"transformer\",\n",
        "    ], \"model_type must be 'attn' or 'transformer'\"\n",
        "    if stocks_train is not None:\n",
        "        assert eval_data is not None, \"No stock evaluation data provided.\"\n",
        "    eval_losses = []\n",
        "    lowest_loss = 1e9\n",
        "\n",
        "    # Move the model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Training on {device}.\")\n",
        "\n",
        "    # If using stock data, we're predicting 5 outomes\n",
        "    if stocks_train is not None:\n",
        "        no_outcomes = 5\n",
        "    else:\n",
        "        no_outcomes = 1\n",
        "\n",
        "    # Get the evaluation data if it is not provided\n",
        "    if eval_data is None:\n",
        "        if linear_data:\n",
        "            eval_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            eval_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "    assert eval_data is not None, \"No evaluation data provided.\"\n",
        "    e_eval = torch.tensor(eval_data[0]).float().to(device)\n",
        "    eval_targets = (\n",
        "        torch.tensor(eval_data[1][:, -no_outcomes:]).float().to(device)\n",
        "    )  # change for stocks\n",
        "\n",
        "    # Define lr scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1000, gamma=0.5)\n",
        "\n",
        "    for step in tqdm(range(training_steps + 1)):\n",
        "        # Generate train data\n",
        "        if stocks_train is not None:\n",
        "            train_data = stocks_train\n",
        "        elif linear_data:\n",
        "            train_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            train_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        e_train = torch.tensor(train_data[0]).float().to(device)\n",
        "        targets = (\n",
        "            torch.tensor(train_data[1][:, -no_outcomes:]).float().to(device)\n",
        "        )  # change for stocks\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        if model_type == \"attn\":\n",
        "            out = model(e_train, e_train, e_train, mask)\n",
        "        else:\n",
        "            out = model(e_train, mask)\n",
        "        preds = out[:, -1, -no_outcomes:] * (-1.0)  # change for stocks\n",
        "        loss = criterion(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate\n",
        "        if step % 100 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                if model_type == \"attn\":\n",
        "                    ev_preds = model(e_eval, e_eval, e_eval)\n",
        "                else:\n",
        "                    ev_preds = model(e_eval, None)  # no mask in evaluation mode\n",
        "                ev_preds = ev_preds[:, -1, -no_outcomes:] * (-1.0)  # change for stocks\n",
        "                eval_loss = criterion(ev_preds, eval_targets)\n",
        "                eval_losses.append(eval_loss)\n",
        "            model.train()\n",
        "            if eval_loss < lowest_loss:\n",
        "                lowest_loss = eval_loss\n",
        "                if stocks_train is not None:\n",
        "                    data_type = \"stocks\"\n",
        "                elif linear_data:\n",
        "                    data_type = \"lin_data\"\n",
        "                else:\n",
        "                    data_type = \"nonlin_data\"\n",
        "                if model_type == \"transformer\":\n",
        "                    att = \"transformer\"\n",
        "                elif model.softmax_att:\n",
        "                    att = \"softmax_attn\"\n",
        "                else:\n",
        "                    att = \"linear_attn\"\n",
        "                path = f\"models/{att}-{data_type}.pth\"\n",
        "                torch.save(model.state_dict(), path)\n",
        "            print(f\"Step {step}, Train Loss: {loss.item():.3f}\")\n",
        "            print(f\"Step {step}, Eval Loss: {eval_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEqqs2G49Kt",
        "outputId": "38b5f38b-38bc-4b93-b7fb-813365214bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 16/1001 [00:00<00:33, 29.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 0.774\n",
            "Step 0, Eval Loss: 0.526\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 128/1001 [00:01<00:06, 136.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 0.152\n",
            "Step 100, Eval Loss: 0.086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 224/1001 [00:02<00:05, 152.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 0.033\n",
            "Step 200, Eval Loss: 0.039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 320/1001 [00:02<00:04, 151.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 0.033\n",
            "Step 300, Eval Loss: 0.023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 417/1001 [00:03<00:03, 156.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 0.029\n",
            "Step 400, Eval Loss: 0.024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 532/1001 [00:04<00:02, 159.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 0.019\n",
            "Step 500, Eval Loss: 0.023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 629/1001 [00:04<00:02, 150.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 0.012\n",
            "Step 600, Eval Loss: 0.021\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 723/1001 [00:05<00:01, 148.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 0.010\n",
            "Step 700, Eval Loss: 0.025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 816/1001 [00:05<00:01, 143.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 0.027\n",
            "Step 800, Eval Loss: 0.036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 929/1001 [00:06<00:00, 149.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 0.008\n",
            "Step 900, Eval Loss: 0.018\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [00:07<00:00, 138.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 0.007\n",
            "Step 1000, Eval Loss: 0.026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Now let's explore training the model\n",
        "import torch.optim as optim\n",
        "\n",
        "# Train\n",
        "optimizer = optim.Adam(mha.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "train(\n",
        "    mha,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=True,\n",
        "    model_type=\"attn\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the previous results we can observe that after just 100 steps of GD, the loss dropped considerably to 0.086. After 600 steps, the model started to overfit.\n",
        "\n",
        "## Non-Linear Data\n",
        "\n",
        "We shall proceed to perform the same experiment as above with a key modification: we shall introduce non-linear data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n",
        "\n",
        "Now we consider contextual data $\\mathcal{C}_m = (x_{m, 1}, y_{m, 1}, \\ldots, x_{m, N}, y_{m, N})$ where in each case $y_{m, i} = f_{w_m}(x_{m, i}) = w_m^T x_{m,i}$, where each $w_m \\sim \\mathcal{N}(\\mathbf{0}_d, I_d)$, where $d = 10$. This is as above, but now the manner with which $x_{m,i}$ are drawn is different: Consider two 10-dimensional real-valued vectors: $v=(v_1, \\ldots, v_{10})^T$ and $u=(u_1, \\ldots, u_{10})^T$, where $v_j = cos(\\frac{j \\pi}{5})$ and $u_j = sin(\\frac{j \\pi}{5})$, for $j = 1, \\ldots, 10$. Each $x_{m,i} = \\alpha v + \\beta u + \\epsilon$, where $\\alpha \\sim \\mathcal{N}(0, 1)$, $\\beta \\sim \\mathcal{N}(0, 1)$, and $\\epsilon = (\\epsilon_1, \\ldots, \\epsilon_{10})^T$ , with $\\epsilon_j \\sim \\mathcal{N}(0, \\frac{1}{100})$.\n",
        "\n",
        "### Weight Initialization\n",
        "\n",
        "The first part of the experiment with non-linear data is to use the same weight construction as we did in the previous exercise. Once we proceed to use softmax attention, we shall drop this construct.\n",
        "\n",
        "### Forward Pass\n",
        "\n",
        "The forward pass of the Transformer model is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ity6FBVo49Kt",
        "outputId": "c0ff641c-3765-46c7-d87d-157c97525b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss pre override for M: 10, N: 1000 is 425.128.\n",
            "Loss with GD weights for M: 10, N: 1000 is 0.432.\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "# Let's do the same but with non linear data\n",
        "eval_nl_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e_eval_nl = torch.tensor(eval_nl_data[0]).float()\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Forward pass pre override\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss pre override for M: {M}, N: {N} is {loss_nl:.3f}.\")\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha_nl.w_q, la_params, \"query\")\n",
        "override_weights(mha_nl.w_k, la_params, \"key\")\n",
        "override_weights(mha_nl.w_v, la_params, \"value\")\n",
        "override_weights(mha_nl.w_o, la_params, \"linear\")\n",
        "\n",
        "# Forward pass\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss with GD weights for M: {M}, N: {N} is {loss_nl:.3f}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quite surprisingly, using the constructed weights from the previous exercise, we were able to achieve a loss of 0.432 on the non-linear data. This suggests that linear self attention was able to capture some of the non-linear relationships in the data.\n",
        "\n",
        "In the same manner as before, we shall proceed to perform a few steps of GD on the designed weights to see if we can achieve better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CSkn2F49Kt",
        "outputId": "043aafa4-9b3d-4844-d9cb-b5693f8eb93a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/3001 [00:00<05:49,  8.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 0.641\n",
            "Step 0, Eval Loss: 5.404\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 103/3001 [00:12<04:49, 10.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 1.100\n",
            "Step 100, Eval Loss: 1.027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 203/3001 [00:23<04:47,  9.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 1.471\n",
            "Step 200, Eval Loss: 1.128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 303/3001 [00:35<04:35,  9.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 2.686\n",
            "Step 300, Eval Loss: 1.232\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 402/3001 [00:47<04:45,  9.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 0.690\n",
            "Step 400, Eval Loss: 1.062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 503/3001 [00:58<04:03, 10.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 1.851\n",
            "Step 500, Eval Loss: 0.996\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 602/3001 [01:16<04:07,  9.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 1.104\n",
            "Step 600, Eval Loss: 1.753\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 701/3001 [01:31<10:57,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 3.782\n",
            "Step 700, Eval Loss: 1.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 803/3001 [01:46<03:46,  9.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 1.036\n",
            "Step 800, Eval Loss: 1.150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 902/3001 [01:58<03:44,  9.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 0.997\n",
            "Step 900, Eval Loss: 1.434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 1002/3001 [02:10<04:03,  8.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 1.066\n",
            "Step 1000, Eval Loss: 1.069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 1102/3001 [02:22<05:31,  5.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1100, Train Loss: 0.867\n",
            "Step 1100, Eval Loss: 1.165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 1202/3001 [02:33<05:09,  5.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1200, Train Loss: 0.582\n",
            "Step 1200, Eval Loss: 1.082\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 1302/3001 [02:44<03:01,  9.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1300, Train Loss: 0.462\n",
            "Step 1300, Eval Loss: 1.036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 1402/3001 [02:55<02:45,  9.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1400, Train Loss: 0.371\n",
            "Step 1400, Eval Loss: 1.039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 1503/3001 [03:07<02:27, 10.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1500, Train Loss: 1.739\n",
            "Step 1500, Eval Loss: 0.987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 1603/3001 [03:18<02:20,  9.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1600, Train Loss: 0.744\n",
            "Step 1600, Eval Loss: 1.002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 1702/3001 [03:30<02:32,  8.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1700, Train Loss: 0.642\n",
            "Step 1700, Eval Loss: 1.103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 1803/3001 [03:41<02:01,  9.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1800, Train Loss: 1.527\n",
            "Step 1800, Eval Loss: 0.970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 1902/3001 [03:52<02:15,  8.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1900, Train Loss: 0.244\n",
            "Step 1900, Eval Loss: 0.986\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 2002/3001 [04:04<01:44,  9.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2000, Train Loss: 0.267\n",
            "Step 2000, Eval Loss: 1.062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 2103/3001 [04:15<01:31,  9.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2100, Train Loss: 0.729\n",
            "Step 2100, Eval Loss: 1.007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 2201/3001 [04:26<01:50,  7.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2200, Train Loss: 0.607\n",
            "Step 2200, Eval Loss: 0.990\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 2302/3001 [04:37<01:52,  6.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2300, Train Loss: 0.905\n",
            "Step 2300, Eval Loss: 1.007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 2402/3001 [04:48<01:06,  9.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2400, Train Loss: 0.412\n",
            "Step 2400, Eval Loss: 0.993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 2502/3001 [04:59<00:52,  9.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2500, Train Loss: 0.339\n",
            "Step 2500, Eval Loss: 1.019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 2601/3001 [05:10<00:39, 10.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2600, Train Loss: 0.194\n",
            "Step 2600, Eval Loss: 1.025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 2702/3001 [05:21<00:29, 10.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2700, Train Loss: 1.356\n",
            "Step 2700, Eval Loss: 0.966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 2802/3001 [05:33<00:21,  9.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2800, Train Loss: 1.046\n",
            "Step 2800, Eval Loss: 1.022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 2902/3001 [05:44<00:10,  9.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2900, Train Loss: 1.530\n",
            "Step 2900, Eval Loss: 0.982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3001/3001 [05:56<00:00,  8.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3000, Train Loss: 0.791\n",
            "Step 3000, Eval Loss: 1.021\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "optimizer = optim.Adam(mha_nl.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 3000\n",
        "\n",
        "# Now let's explore training the model\n",
        "train(\n",
        "    mha_nl,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_nl_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results from the training is quite baffling: the model wasn't able to converge into a state that generalized better than the manual weight construction. It immediately left that local optima and never returned. I experimented with smaller learning rates, larger N but the results were the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Softmax Attention\n",
        "\n",
        "We shall now proceed to implement the softmax attention mechanism and evaluate the model on the non-linear data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqMN7ZO049Kt",
        "outputId": "71e6e79e-cf27-477d-8cb3-22c16dc0ee8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1001 [00:00<02:02,  8.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 1.214\n",
            "Step 0, Eval Loss: 0.925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 102/1001 [00:11<01:43,  8.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 0.657\n",
            "Step 100, Eval Loss: 0.816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 203/1001 [00:22<01:19, 10.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 0.845\n",
            "Step 200, Eval Loss: 0.781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 302/1001 [00:34<01:20,  8.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 1.154\n",
            "Step 300, Eval Loss: 0.767\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 402/1001 [00:45<01:41,  5.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 0.835\n",
            "Step 400, Eval Loss: 0.775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 503/1001 [00:55<00:48, 10.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 0.927\n",
            "Step 500, Eval Loss: 0.796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 602/1001 [01:06<00:40,  9.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 1.260\n",
            "Step 600, Eval Loss: 0.743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 703/1001 [01:18<00:30,  9.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 0.980\n",
            "Step 700, Eval Loss: 0.664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 802/1001 [01:29<00:21,  9.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 0.829\n",
            "Step 800, Eval Loss: 0.620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 902/1001 [01:41<00:11,  8.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 0.726\n",
            "Step 900, Eval Loss: 0.647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [01:53<00:00,  8.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 1.069\n",
            "Step 1000, Eval Loss: 0.662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Finally let's use softmax attention\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl_sa = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "optimizer = optim.Adam(mha_nl_sa.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    mha_nl_sa,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can observe that the model was able to outperform the learning process of the linear self attention mechanism. The loss dropped to 0.62 before starting to show signs of overfitting. We shall now proceed to evaluate the same experiment using a full Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Traditional Transformer\n",
        "\n",
        "As a final task before moving onto stock prediction, we shall attempt to improve on the previous results using the full Transformer architecture. To our model, we add Layer Normalization, a Multi-Layer Perceptron, and Residual Connections. Note that we shall still continue to use a single attention head. The reason is because of our token construction: 10 dimensions + output = 11 input dimensions. This isn't divisible by any number of heads at the moment. We shall later proceed to make some modifications but we won't be able to directly compare with the previuos experiments once we do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtnZfAP0M4a6"
      },
      "outputs": [],
      "source": [
        "from scratch_transformer import (\n",
        "    LayerNormalization,\n",
        "    FeedForwardBlock,\n",
        "    ResidualConnection,\n",
        "    EncoderBlock,\n",
        "    MultiHeadAttentionBlock,\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from data import get_nonlinear_data\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-4\n",
        "dropout = 0.2\n",
        "mask = None\n",
        "\n",
        "# get the data\n",
        "eval_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "\n",
        "# MLP dimension usually 4 times the d_model\n",
        "# Residual connection already contains layer normalizations\n",
        "\n",
        "# Start with Self Attention\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=dropout, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "\n",
        "# Feed Forward\n",
        "ff = FeedForwardBlock(\n",
        "    d_model=feature_size + 1, d_ff=4 * (feature_size + 1), dropout=dropout\n",
        ")  # (batch_size, seq_len, d_model\n",
        "\n",
        "\n",
        "# Create an EncoderBlock\n",
        "eb = EncoderBlock(\n",
        "    self_attention_block=mha,\n",
        "    feed_forward_block=ff,\n",
        "    dropout=dropout,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj8a7p3jM4a7",
        "outputId": "f874cb33-5743-45f9-a129-b6173e832307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 1555\n",
            "Training on cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/1001 [00:01<07:23,  2.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 18.331\n",
            "Step 0, Eval Loss: 4.704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 102/1001 [00:12<01:40,  8.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 3.360\n",
            "Step 100, Eval Loss: 4.210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 203/1001 [00:24<01:19, 10.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 2.359\n",
            "Step 200, Eval Loss: 3.661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 302/1001 [00:36<01:19,  8.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 29.277\n",
            "Step 300, Eval Loss: 3.126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 403/1001 [00:47<01:01,  9.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 8.752\n",
            "Step 400, Eval Loss: 2.586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 502/1001 [00:59<00:55,  8.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 3.398\n",
            "Step 500, Eval Loss: 2.059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 601/1001 [01:10<00:44,  8.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 8.046\n",
            "Step 600, Eval Loss: 1.619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 701/1001 [01:22<00:31,  9.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 17.576\n",
            "Step 700, Eval Loss: 1.217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 802/1001 [01:33<00:32,  6.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 9.904\n",
            "Step 800, Eval Loss: 0.878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 902/1001 [01:45<00:16,  5.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 2.503\n",
            "Step 900, Eval Loss: 0.605\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [01:56<00:00,  8.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 1.350\n",
            "Step 1000, Eval Loss: 0.442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_steps = 1000\n",
        "optimizer = optim.Adam(eb.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in eb.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    eb,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        "    model_type=\"transformer\",\n",
        "    mask=mask,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can quickly observe that the full architecture was able to achieve a loss 33% lower than the previous softmax attention mechanism in the same number of training steps and with a negligible increase in training time. \n",
        "\n",
        "Given the successful implementation of different components of the Transformer architecture, we proceed to evaluate the model on stock data.\n",
        "\n",
        "## Stock Prediction\n",
        "\n",
        "We shall now proceed to evaluate the Transformer model on stock data. We shall compare the performance of the Transformer model with popular, powerful machine learning models such as the LSTM and XGBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data\n",
        "\n",
        "The SPY ETF is an exchange-traded fund that tracks the S&P 500 index. It is one of the most widely traded ETFs in the world and provides exposure to the U.S. stock market.\n",
        "\n",
        "We gathered daily historical Open, High, Low, Close and 3 Month Implied Volatility data for the SPY ETF from May 2014 to March 2024.\n",
        "\n",
        "### Data Preprocessing\n",
        "\n",
        "To begin with, I scaled the prices by 1/200 and performed a log transformation on the prices and implied volatility so that the inputs varied around zero.\n",
        "\n",
        "We then created 1 day, 5 day, 10 day, 20 day Future Closing prices as well as 1 day Future Open price to serve as our target variables. We proceeded to create context windows $C_m = x_{m,1}, y_{m,1} \\ldots x_{m,N}, y_{m,N} $ of size 230 days and an additional query $x_{m, N+1}$ to be used in an attempt to predict $y_{m, N+1}$. Note from this construct that this implies our goal is to predict 5 separate target variables. Otherwise, the construct is similar to that of the previous sections.\n",
        "\n",
        "Since we're dealing with sequential data, we slide the context window across our entire dataset in order to generate our training and testing data. Using a context window instead of simply separating the entire dataset into two train & test pieces allows our model to learn from different contexts provided by the different market environments.\n",
        "\n",
        "In addition to $y_{m, N+1}$ and with the purpose of avoiding lookahead bias, we proceeded to separate the following 20 days of each context $C_m$ as the test set. The reason being that one of our target variables is the future 20 day Closing Price.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pmsfV11IkBEN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from data import get_stock_data\n",
        "\n",
        "path = \"data/stocks.csv\"\n",
        "spy_train, spy_eval = get_stock_data(path, ticker=\"SPY\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "We now proceed to construct the Transformer model using the full architecture utilized in the last experiment. We now increase the number of attention heads to 5 and utilize a total of 12 layers. The total number of parameters increased 10x with respect to the previous implementation. To help manage the additional complexity, we shall implement a dropout rate of 20% and a Learning Rate Scheduler. Additionally, we shall allow 5,000 steps of training as I expect this model to take longer to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqJUj-V4M4a8"
      },
      "outputs": [],
      "source": [
        "from scratch_transformer import (\n",
        "    LayerNormalization,\n",
        "    FeedForwardBlock,\n",
        "    ResidualConnection,\n",
        "    EncoderBlock,\n",
        "    MultiHeadAttentionBlock,\n",
        "    Encoder,\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "dropout = 0.2\n",
        "mask = None\n",
        "heads = 5\n",
        "layers = 12\n",
        "\n",
        "# convert to tensor\n",
        "e = torch.tensor(spy_eval[0]).float()\n",
        "et = torch.tensor(spy_train[0]).float()\n",
        "\n",
        "\n",
        "# MLP dimension usually 4 times the d_model\n",
        "# Residual connection already contains layer normalizations\n",
        "\n",
        "# Start with Self Attention\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size,\n",
        "    heads=heads,\n",
        "    dropout=dropout,\n",
        "    softmax_att=True,\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# out = mha(e, e, e)\n",
        "out = mha(et, et, et, mask)\n",
        "\n",
        "# Feed Forward\n",
        "ff = FeedForwardBlock(\n",
        "    d_model=feature_size, d_ff=4 * feature_size, dropout=dropout\n",
        ")  # (batch_size, seq_len, d_model\n",
        "\n",
        "\n",
        "# Create an EncoderBlock\n",
        "eb = EncoderBlock(\n",
        "    self_attention_block=mha,\n",
        "    feed_forward_block=ff,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "encoder_blocks = []\n",
        "for _ in range(layers):\n",
        "    encoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "        d_model=feature_size, heads=heads, dropout=dropout, softmax_att=True\n",
        "    )\n",
        "    encoder_feed_forward_block = FeedForwardBlock(\n",
        "        d_model=feature_size, d_ff=4 * feature_size, dropout=dropout\n",
        "    )\n",
        "    encoder_block = EncoderBlock(\n",
        "        self_attention_block=encoder_self_attention_block,\n",
        "        feed_forward_block=encoder_feed_forward_block,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "# Don't worry about Encoder, it's just predefined in scratch_transformer\n",
        "decoder = Encoder(\n",
        "    nn.ModuleList(encoder_blocks),\n",
        ")\n",
        "\n",
        "# out = decoder(et, mask)\n",
        "\n",
        "for p in decoder.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "# Compare the output to the targets\n",
        "# eval_targets = spy_eval[1]\n",
        "# eval_preds = out[:, -1, -5:] * (-1.0)\n",
        "\n",
        "# loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "# print(f\"Loss is {loss:.3f}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqDM_1krM4a8",
        "outputId": "1020d7f2-1b05-420d-f799-e510a6332df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of parameters: 15530\n",
            "Training on cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/5001 [00:00<05:31, 15.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 0.865\n",
            "Step 0, Eval Loss: 0.544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 104/5001 [00:04<03:45, 21.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 0.074\n",
            "Step 100, Eval Loss: 0.055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 203/5001 [00:08<03:40, 21.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 0.058\n",
            "Step 200, Eval Loss: 0.045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 305/5001 [00:14<03:27, 22.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 0.040\n",
            "Step 300, Eval Loss: 0.013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 404/5001 [00:18<03:17, 23.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 0.017\n",
            "Step 400, Eval Loss: 0.013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 504/5001 [00:23<05:21, 13.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 0.019\n",
            "Step 500, Eval Loss: 0.008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 603/5001 [00:27<03:15, 22.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 0.016\n",
            "Step 600, Eval Loss: 0.008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 705/5001 [00:31<03:08, 22.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 0.011\n",
            "Step 700, Eval Loss: 0.007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 805/5001 [00:36<03:12, 21.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 0.008\n",
            "Step 800, Eval Loss: 0.007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 904/5001 [00:41<03:07, 21.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 0.011\n",
            "Step 900, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1003/5001 [00:45<02:59, 22.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 0.010\n",
            "Step 1000, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 1103/5001 [00:50<02:51, 22.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1100, Train Loss: 0.009\n",
            "Step 1100, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 1205/5001 [00:54<02:59, 21.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1200, Train Loss: 0.008\n",
            "Step 1200, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 1305/5001 [01:00<03:25, 17.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1300, Train Loss: 0.007\n",
            "Step 1300, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 1405/5001 [01:04<02:46, 21.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1400, Train Loss: 0.016\n",
            "Step 1400, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 1504/5001 [01:09<02:36, 22.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1500, Train Loss: 0.020\n",
            "Step 1500, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 1603/5001 [01:14<02:30, 22.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1600, Train Loss: 0.006\n",
            "Step 1600, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 1705/5001 [01:18<02:26, 22.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1700, Train Loss: 0.006\n",
            "Step 1700, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 1803/5001 [01:22<03:01, 17.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1800, Train Loss: 0.016\n",
            "Step 1800, Eval Loss: 0.007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 1905/5001 [01:27<02:08, 24.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1900, Train Loss: 0.005\n",
            "Step 1900, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2004/5001 [01:31<02:04, 23.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2000, Train Loss: 0.012\n",
            "Step 2000, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 2104/5001 [01:36<02:38, 18.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2100, Train Loss: 0.007\n",
            "Step 2100, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 2203/5001 [01:40<02:02, 22.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2200, Train Loss: 0.006\n",
            "Step 2200, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 2305/5001 [01:44<01:53, 23.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2300, Train Loss: 0.007\n",
            "Step 2300, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 2405/5001 [01:49<01:49, 23.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2400, Train Loss: 0.005\n",
            "Step 2400, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 2504/5001 [01:53<01:45, 23.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2500, Train Loss: 0.005\n",
            "Step 2500, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 2603/5001 [01:58<02:18, 17.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2600, Train Loss: 0.005\n",
            "Step 2600, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 2705/5001 [02:03<01:38, 23.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2700, Train Loss: 0.004\n",
            "Step 2700, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 2804/5001 [02:07<01:33, 23.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2800, Train Loss: 0.005\n",
            "Step 2800, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 2902/5001 [02:12<02:20, 14.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2900, Train Loss: 0.008\n",
            "Step 2900, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3005/5001 [02:16<01:22, 24.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3000, Train Loss: 0.004\n",
            "Step 3000, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 3104/5001 [02:21<01:21, 23.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3100, Train Loss: 0.005\n",
            "Step 3100, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 3205/5001 [02:25<01:16, 23.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3200, Train Loss: 0.004\n",
            "Step 3200, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 3304/5001 [02:30<01:11, 23.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3300, Train Loss: 0.003\n",
            "Step 3300, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 3403/5001 [02:34<01:08, 23.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3400, Train Loss: 0.005\n",
            "Step 3400, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 3504/5001 [02:39<01:02, 23.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3500, Train Loss: 0.006\n",
            "Step 3500, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 3603/5001 [02:43<00:59, 23.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3600, Train Loss: 0.005\n",
            "Step 3600, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 3703/5001 [02:47<01:12, 17.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3700, Train Loss: 0.004\n",
            "Step 3700, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 3806/5001 [02:52<00:50, 23.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3800, Train Loss: 0.005\n",
            "Step 3800, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 3905/5001 [02:56<00:51, 21.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 3900, Train Loss: 0.004\n",
            "Step 3900, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4003/5001 [03:02<00:44, 22.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4000, Train Loss: 0.006\n",
            "Step 4000, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 4105/5001 [03:07<00:38, 23.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4100, Train Loss: 0.003\n",
            "Step 4100, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 4204/5001 [03:12<00:46, 17.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4200, Train Loss: 0.003\n",
            "Step 4200, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 4305/5001 [03:16<00:29, 23.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4300, Train Loss: 0.003\n",
            "Step 4300, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 4404/5001 [03:20<00:25, 23.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4400, Train Loss: 0.003\n",
            "Step 4400, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 4505/5001 [03:25<00:25, 19.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4500, Train Loss: 0.004\n",
            "Step 4500, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 4604/5001 [03:29<00:16, 23.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4600, Train Loss: 0.003\n",
            "Step 4600, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 4705/5001 [03:34<00:12, 23.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4700, Train Loss: 0.004\n",
            "Step 4700, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 4805/5001 [03:39<00:08, 23.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4800, Train Loss: 0.007\n",
            "Step 4800, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 4904/5001 [03:43<00:04, 22.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 4900, Train Loss: 0.004\n",
            "Step 4900, Eval Loss: 0.005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5001/5001 [03:47<00:00, 21.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 5000, Train Loss: 0.002\n",
            "Step 5000, Eval Loss: 0.006\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Train\n",
        "lr = 1e-3\n",
        "training_steps = 5000\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in decoder.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    decoder,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=spy_eval,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        "    model_type=\"transformer\",\n",
        "    mask=mask,\n",
        "    stocks_train=spy_train,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the model begins to show signs of overfitting around step 4,500. Given that we're utilizing log-transformed prices, looking at the Eval Loss by itself doesn't provide much information. We shall now proceed to fit the LSTM and XGBoost models to the same data and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IEc9n2SM4a8",
        "outputId": "e979704c-766d-46bc-dcc3-5170adb0c18e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Loss: 0.005\n"
          ]
        }
      ],
      "source": [
        "# load the transformer-stocks.pth model\n",
        "decoder.load_state_dict(torch.load(\"models/transformer-stocks.pth\"))\n",
        "criterion = torch.nn.MSELoss()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "decoder.to(device)\n",
        "# Evaluate the model\n",
        "decoder.eval()\n",
        "with torch.no_grad():\n",
        "    e_eval = torch.tensor(spy_eval[0]).float()\n",
        "    eval_preds = decoder(e_eval.to(device), None)[:, -1, -5:] * (-1.0)\n",
        "    eval_targets = torch.tensor(spy_eval[1]).float()\n",
        "    eval_loss = criterion(eval_preds, eval_targets.to(device))\n",
        "\n",
        "print(f\"Eval Loss: {eval_loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data pre-processing\n",
        "\n",
        "In order to fit the LSTM and XGBoost models, we shall construct train and test data in the traditional construct of machine learning models. Note that this implies that the dimensions of the input and our testing data will now be 5 each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1ZqqXlfNkBEO"
      },
      "outputs": [],
      "source": [
        "# Now we change the inputs to traditional x, y\n",
        "train = spy_train[0].reshape(-1, 10)\n",
        "test = spy_eval[0].reshape(-1, 10)\n",
        "\n",
        "x_train = train[:, :5]\n",
        "y_train = train[:, 5:]\n",
        "\n",
        "x_test = test[:, :5]\n",
        "y_test = test[:, 5:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost\n",
        "\n",
        "We begin by fitting an XGBoost Regressor and calculating its MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_-rmyJoM4a8",
        "outputId": "a30c7b6b-1b67-4805-abc4-66e76bb391c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Test MSE: 0.013\n"
          ]
        }
      ],
      "source": [
        "# compare to XGBoost\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create the model\n",
        "model = XGBRegressor(\n",
        "    n_estimators=10000,\n",
        "    max_depth=100,\n",
        "    learning_rate=0.01,\n",
        "    objective=\"reg:squarederror\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Calculate the MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"XGBoost Test MSE: {mse:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can observe, at 0.013 the loss is considerably higher than the 0.005 achieved by the Transformer. As a first result, this is very encouraging towards evaluating our initial hypothesis.\n",
        "\n",
        "We now proceed to train and evaluate an LSTM on the same data.\n",
        "\n",
        "### LSTM\n",
        "\n",
        "Similar to the traditional construct of the MLP in the Transformer, we shall choose a hidden size that is 4x the input's dimensions. Additionally, we have chosen the same number of layers as that which we used in the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "US6Ddbh4kBEO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "feature_size = 5\n",
        "output_size = 1\n",
        "hidden_size = 20\n",
        "num_layers = 12\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size, hidden_size, num_layers, batch_first=True, dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "model = LSTMModel(feature_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Initialize the weights of the model\n",
        "for name, param in model.named_parameters():\n",
        "    if \"bias\" in name:\n",
        "        nn.init.constant_(param, 0.0)\n",
        "    elif \"weight\" in name:\n",
        "        nn.init.xavier_uniform_(param)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uEoXdFMkBEO",
        "outputId": "c84056f4-dc8e-454c-d261-22e56bc0efbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Epoch 1, Training loss: 0.2050805888981719, Validation loss: 0.10673071444034576\n",
            "Epoch 101, Training loss: 0.02208308392035743, Validation loss: 0.02183053269982338\n",
            "Epoch 201, Training loss: 0.008579795744575175, Validation loss: 0.017444998025894165\n",
            "Epoch 301, Training loss: 0.00848640529097636, Validation loss: 0.01739390194416046\n",
            "Epoch 401, Training loss: 0.007310390781514861, Validation loss: 0.019773859530687332\n",
            "Epoch 501, Training loss: 0.006472167184632593, Validation loss: 0.016421332955360413\n",
            "Epoch 601, Training loss: 0.005802895442245464, Validation loss: 0.019048817455768585\n",
            "Epoch 701, Training loss: 0.006011815845026988, Validation loss: 0.02138584852218628\n",
            "Epoch 801, Training loss: 0.005568012486790018, Validation loss: 0.016140814870595932\n",
            "Epoch 901, Training loss: 0.005295451493644765, Validation loss: 0.01866176165640354\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {device}.\")\n",
        "\n",
        "\n",
        "# Convert the data to tensors\n",
        "x_train_tensor = torch.tensor(x_train).float().to(device)\n",
        "y_train_tensor = torch.tensor(y_train).float().to(device)\n",
        "x_test_tensor = torch.tensor(x_test).float().to(device)\n",
        "y_test_tensor = torch.tensor(y_test).float().to(device)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "best_val_loss = 1e9\n",
        "num_epochs = 1000\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(x_test_tensor)\n",
        "            val_loss = criterion(val_outputs, y_test_tensor)\n",
        "        print(\n",
        "            f\"Epoch {epoch + 1}, Training loss: {running_loss / len(train_loader)}, Validation loss: {val_loss.item()}\"\n",
        "        )\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"models/lstm-stocks.pth\")\n",
        "        model.train()\n",
        "\n",
        "print(\"Finished Training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Somewhat underwhelming results from the LSTM: with a loss of 0.016, it was the worst performer of our 3 experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "Overall I consider the results to be very encouraging when contemplating whether Transformers can belong in an investment advisor's toolkit. \n",
        "\n",
        "Of first note, the features utilized in this experiment are as simple as can be. I argue that this was done by construction in order to demonstrate the capabilities of a Transformer to extract and design meaningful features and lay a solid foundation for future work. As it exists today, there is a plethora of financial engineering literature dedicated to creating signals that allow traders to gain the slightest edge in the market. The next step in the evolution of this experiment is to start integrating such features.\n",
        "\n",
        "Another potentially exciting avenue to explore is the utilization of sentiment analysis data. With the advent of LLMs, literature and tools on sentiment analysis has exploded. Given the Transformer's capacity to gather context from features, I think it could be immensely valuable to integrate sentiment related features to the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., & Vladymyrov, M. (2023). Transformers learn in-context by gradient descent. arXiv:2212.07677v2. https://doi.org/10.48550/arXiv.2212.076772\n",
        "- Garg, S., Tsipras, D., Liang, P., & Valiant, G. (2023). What Can Transformers Learn In-Context? A Case Study of Simple Function Classes. arXiv:2208.01066v3. https://doi.org/10.48550/arXiv.2208.01066\n",
        "- Gruver, N., Finzi, M., Qiu, S., & Wilson, A. G. (2023). Large Language Models Are Zero-Shot Time Series Forecasters arXiv preprint arXiv:2310.078201. Retrieved from https://doi.org/10.48550/arXiv.2310.07820\n",
        "- Karpathy, Andrej (2022). nanoGPT, The simplest, fastest repository for training/finetuning medium-sized GPTs. https://github.com/karpathy/nanoGPT/tree/master?tab=MIT-1-ov-file#readme\n",
        "- hkproj (2023). Coding a Transformer from scratch on PyTorch. Attention is all you need implementation. https://github.com/hkproj/pytorch-transformer?tab=readme-ov-file\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
