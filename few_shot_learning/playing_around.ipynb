{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLOp3W1o5ENS",
        "outputId": "3daf093c-cca0-41b1-f580-373b2db1b2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 74 (delta 29), reused 61 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 8.98 MiB | 26.14 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/transformers/few_shot_learning\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/cervs257/transformers\n",
        "# %cd transformers/few_shot_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Ba9jIIhI49Kr"
      },
      "outputs": [],
      "source": [
        "from scratch_transformer import MultiHeadAttentionBlock\n",
        "from data import create_weights, get_reg_data, get_nonlinear_data\n",
        "import numpy as np\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-3\n",
        "\n",
        "# linear attention params override\n",
        "la_params = create_weights(feature_size, output_size, N, lr)\n",
        "\n",
        "# get the data\n",
        "eval_data = get_reg_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JIeEmuz949Ks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Now we will override the weights of the model to implement those that perform GD in the forward pass\n",
        "def override_weights(model, new_params, w_name):\n",
        "    w_name = \"Transformer_gd/multi_head_attention/\" + w_name\n",
        "    w_numpy = new_params[w_name][\"w\"]\n",
        "    w_tensor = torch.tensor(w_numpy, dtype=model.weight.dtype)\n",
        "    model.weight.data = w_tensor\n",
        "\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha.w_q, la_params, \"query\")\n",
        "override_weights(mha.w_k, la_params, \"key\")\n",
        "override_weights(mha.w_v, la_params, \"value\")\n",
        "override_weights(mha.w_o, la_params, \"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "JnmTyGrv49Ks"
      },
      "outputs": [],
      "source": [
        "def compute_loss(preds, targets):\n",
        "    \"\"\"Compute the MSE loss.\"\"\"\n",
        "    return 0.5 * np.sum((targets - preds) ** 2) / targets.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0fnTyjvR49Ks"
      },
      "outputs": [],
      "source": [
        "e_eval = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "# Forward pass\n",
        "out = mha(e_eval, e_eval, e_eval)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_targets = eval_data[1][:, -1]\n",
        "eval_preds = out[:, -1, -1] * (-1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC5hxQxF49Ks",
        "outputId": "5c5d898b-b031-4536-9f8f-f30e37352abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for M: 10, N: 1000 is 0.484.\n"
          ]
        }
      ],
      "source": [
        "loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "print(f\"Loss for M: {M}, N: {N} is {loss:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6xxrFe7R49Kt"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=1000,\n",
        "    linear_data=False,\n",
        "    model_type=\"attn\",\n",
        "    mask=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    param model_type: str, \"attn\" or \"transformer\"\n",
        "    \"\"\"\n",
        "    assert model_type in [\n",
        "        \"attn\",\n",
        "        \"transformer\",\n",
        "    ], \"model_type must be 'attn' or 'transformer'\"\n",
        "    eval_losses = []\n",
        "    lowest_loss = 1e9\n",
        "\n",
        "    # Move the model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Training on {device}.\")\n",
        "\n",
        "    # Get the evaluation data if it is not provided\n",
        "    if eval_data is None:\n",
        "        if linear_data:\n",
        "            eval_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            eval_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "    assert eval_data is not None, \"No evaluation data provided.\"\n",
        "    e_eval = torch.tensor(eval_data[0]).float().to(device)\n",
        "    eval_targets = torch.tensor(eval_data[1][:, -1]).float().to(device)\n",
        "\n",
        "    # Define lr scheduler\n",
        "    scheduler = StepLR(optimizer, step_size=1000, gamma=0.5)\n",
        "\n",
        "    for step in tqdm(range(training_steps + 1)):\n",
        "        # Generate train data\n",
        "        if linear_data:\n",
        "            train_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            train_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        e_train = torch.tensor(train_data[0]).float().to(device)\n",
        "        targets = torch.tensor(train_data[1][:, -1]).float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        if model_type == \"attn\":\n",
        "            out = model(e_train, e_train, e_train)\n",
        "        else:\n",
        "            out = model(e_train, mask)\n",
        "        preds = out[:, -1, -1] * (-1.0)\n",
        "        loss = criterion(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Evaluate\n",
        "        if step % 100 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                if model_type == \"attn\":\n",
        "                    ev_preds = model(e_eval, e_eval, e_eval)\n",
        "                else:\n",
        "                    ev_preds = model(e_eval, mask)\n",
        "                ev_preds = ev_preds[:, -1, -1] * (-1.0)\n",
        "                eval_loss = criterion(ev_preds, eval_targets)\n",
        "                eval_losses.append(eval_loss)\n",
        "            model.train()\n",
        "            if eval_loss < lowest_loss:\n",
        "                lowest_loss = eval_loss\n",
        "                if linear_data:\n",
        "                    data_type = \"lin_data\"\n",
        "                else:\n",
        "                    data_type = \"nonlin_data\"\n",
        "                if model_type == \"transformer\":\n",
        "                    att = \"transformer\"\n",
        "                elif model.softmax_att:\n",
        "                    att = \"softmax_attn\"\n",
        "                else:\n",
        "                    att = \"linear_attn\"\n",
        "                path = f\"models/{att}-{data_type}.pth\"\n",
        "                torch.save(model.state_dict(), path)\n",
        "            print(f\"Step {step}, Train Loss: {loss.item():.3f}\")\n",
        "            print(f\"Step {step}, Eval Loss: {eval_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEqqs2G49Kt",
        "outputId": "977e0c6f-f9f8-4e35-aff5-039a00562fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/1001 [00:00<01:59,  8.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0, Train Loss: 2.049\n",
            "Step 0, Eval Loss: 0.908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 104/1001 [00:05<00:44, 19.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 100, Train Loss: 0.101\n",
            "Step 100, Eval Loss: 0.040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 203/1001 [00:10<00:40, 19.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200, Train Loss: 0.111\n",
            "Step 200, Eval Loss: 0.011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 302/1001 [00:14<00:34, 19.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 300, Train Loss: 0.020\n",
            "Step 300, Eval Loss: 0.007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 403/1001 [00:19<00:28, 20.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 400, Train Loss: 0.013\n",
            "Step 400, Eval Loss: 0.015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 502/1001 [00:24<00:24, 20.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500, Train Loss: 0.016\n",
            "Step 500, Eval Loss: 0.016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 604/1001 [00:29<00:17, 22.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 600, Train Loss: 0.030\n",
            "Step 600, Eval Loss: 0.008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 703/1001 [00:34<00:14, 19.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 700, Train Loss: 0.005\n",
            "Step 700, Eval Loss: 0.011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 805/1001 [00:38<00:09, 21.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 800, Train Loss: 0.028\n",
            "Step 800, Eval Loss: 0.015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 904/1001 [00:43<00:04, 21.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 900, Train Loss: 0.018\n",
            "Step 900, Eval Loss: 0.011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1001/1001 [00:47<00:00, 21.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000, Train Loss: 0.009\n",
            "Step 1000, Eval Loss: 0.015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Now let's explore training the model\n",
        "import torch.optim as optim\n",
        "\n",
        "# Train\n",
        "optimizer = optim.Adam(mha.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "train(\n",
        "    mha,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=True,\n",
        "    model_type=\"attn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ity6FBVo49Kt",
        "outputId": "ad566c23-bf13-4eb0-b022-018aabddb0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss pre override for M: 10, N: 1000 is 1059.016.\n",
            "Loss with GD weights for M: 10, N: 1000 is 0.603.\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "# Let's do the same but with non linear data\n",
        "eval_nl_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e_eval_nl = torch.tensor(eval_nl_data[0]).float()\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Forward pass pre override\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss pre override for M: {M}, N: {N} is {loss_nl:.3f}.\")\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha_nl.w_q, la_params, \"query\")\n",
        "override_weights(mha_nl.w_k, la_params, \"key\")\n",
        "override_weights(mha_nl.w_v, la_params, \"value\")\n",
        "override_weights(mha_nl.w_o, la_params, \"linear\")\n",
        "\n",
        "# Forward pass\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss with GD weights for M: {M}, N: {N} is {loss_nl:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CSkn2F49Kt",
        "outputId": "86db883d-4d89-41c1-bde0-726f5592028b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 1.222\n",
            "Step 0, Eval Loss: 8.459\n",
            "Step 100, Train Loss: 1.940\n",
            "Step 100, Eval Loss: 2.947\n",
            "Step 200, Train Loss: 1.526\n",
            "Step 200, Eval Loss: 1.124\n",
            "Step 300, Train Loss: 2.347\n",
            "Step 300, Eval Loss: 1.023\n",
            "Step 400, Train Loss: 1.603\n",
            "Step 400, Eval Loss: 1.079\n",
            "Step 500, Train Loss: 1.361\n",
            "Step 500, Eval Loss: 1.208\n",
            "Step 600, Train Loss: 2.761\n",
            "Step 600, Eval Loss: 1.065\n",
            "Step 700, Train Loss: 0.738\n",
            "Step 700, Eval Loss: 1.183\n",
            "Step 800, Train Loss: 1.057\n",
            "Step 800, Eval Loss: 1.669\n",
            "Step 900, Train Loss: 2.104\n",
            "Step 900, Eval Loss: 1.173\n",
            "Step 1000, Train Loss: 1.093\n",
            "Step 1000, Eval Loss: 1.194\n",
            "Step 1100, Train Loss: 0.666\n",
            "Step 1100, Eval Loss: 1.440\n",
            "Step 1200, Train Loss: 1.182\n",
            "Step 1200, Eval Loss: 1.303\n",
            "Step 1300, Train Loss: 1.037\n",
            "Step 1300, Eval Loss: 1.566\n",
            "Step 1400, Train Loss: 1.827\n",
            "Step 1400, Eval Loss: 1.525\n",
            "Step 1500, Train Loss: 0.792\n",
            "Step 1500, Eval Loss: 1.274\n",
            "Step 1600, Train Loss: 1.173\n",
            "Step 1600, Eval Loss: 1.125\n",
            "Step 1700, Train Loss: 1.082\n",
            "Step 1700, Eval Loss: 1.044\n",
            "Step 1800, Train Loss: 0.858\n",
            "Step 1800, Eval Loss: 1.007\n",
            "Step 1900, Train Loss: 0.447\n",
            "Step 1900, Eval Loss: 0.985\n",
            "Step 2000, Train Loss: 1.200\n",
            "Step 2000, Eval Loss: 1.018\n",
            "Step 2100, Train Loss: 1.545\n",
            "Step 2100, Eval Loss: 1.113\n",
            "Step 2200, Train Loss: 1.016\n",
            "Step 2200, Eval Loss: 1.070\n",
            "Step 2300, Train Loss: 0.966\n",
            "Step 2300, Eval Loss: 1.090\n",
            "Step 2400, Train Loss: 0.610\n",
            "Step 2400, Eval Loss: 1.077\n",
            "Step 2500, Train Loss: 0.830\n",
            "Step 2500, Eval Loss: 0.959\n",
            "Step 2600, Train Loss: 0.696\n",
            "Step 2600, Eval Loss: 0.933\n",
            "Step 2700, Train Loss: 0.993\n",
            "Step 2700, Eval Loss: 1.157\n",
            "Step 2800, Train Loss: 0.278\n",
            "Step 2800, Eval Loss: 1.183\n",
            "Step 2900, Train Loss: 0.645\n",
            "Step 2900, Eval Loss: 0.987\n",
            "Step 3000, Train Loss: 1.080\n",
            "Step 3000, Eval Loss: 1.197\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "optimizer = optim.Adam(mha_nl.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 3000\n",
        "\n",
        "# Now let's explore training the model\n",
        "train(\n",
        "    mha_nl,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_nl_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqMN7ZO049Kt",
        "outputId": "c20dadb0-ed18-46cc-af6c-dc74d32f8e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 0.778\n",
            "Step 0, Eval Loss: 0.747\n",
            "Step 100, Train Loss: 0.854\n",
            "Step 100, Eval Loss: 0.677\n",
            "Step 200, Train Loss: 0.770\n",
            "Step 200, Eval Loss: 0.567\n",
            "Step 300, Train Loss: 0.759\n",
            "Step 300, Eval Loss: 0.455\n",
            "Step 400, Train Loss: 0.794\n",
            "Step 400, Eval Loss: 0.432\n",
            "Step 500, Train Loss: 0.550\n",
            "Step 500, Eval Loss: 0.414\n",
            "Step 600, Train Loss: 0.581\n",
            "Step 600, Eval Loss: 0.449\n",
            "Step 700, Train Loss: 1.445\n",
            "Step 700, Eval Loss: 0.418\n",
            "Step 800, Train Loss: 0.475\n",
            "Step 800, Eval Loss: 0.403\n",
            "Step 900, Train Loss: 0.499\n",
            "Step 900, Eval Loss: 0.399\n",
            "Step 1000, Train Loss: 0.486\n",
            "Step 1000, Eval Loss: 0.427\n"
          ]
        }
      ],
      "source": [
        "# Finally let's use softmax attention\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl_sa = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "optimizer = optim.Adam(mha_nl_sa.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    mha_nl_sa,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scratch_transformer import (\n",
        "    LayerNormalization,\n",
        "    FeedForwardBlock,\n",
        "    ResidualConnection,\n",
        "    EncoderBlock,\n",
        "    MultiHeadAttentionBlock,\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from data import get_nonlinear_data\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-4\n",
        "dropout = 0.2\n",
        "mask = None\n",
        "\n",
        "# get the data\n",
        "eval_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "\n",
        "# MLP dimension usually 4 times the d_model\n",
        "# Residual connection already contains layer normalizations\n",
        "\n",
        "# Start with Self Attention\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=dropout, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "\n",
        "# Feed Forward\n",
        "ff = FeedForwardBlock(\n",
        "    d_model=feature_size + 1, d_ff=4 * (feature_size + 1), dropout=dropout\n",
        ")  # (batch_size, seq_len, d_model\n",
        "\n",
        "\n",
        "# Create an EncoderBlock\n",
        "eb = EncoderBlock(\n",
        "    self_attention_block=mha,\n",
        "    feed_forward_block=ff,\n",
        "    dropout=dropout,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cpu.\n",
            "Step 0, Train Loss: 37.812\n",
            "Step 0, Eval Loss: 5.308\n",
            "Step 100, Train Loss: 12.215\n",
            "Step 100, Eval Loss: 5.267\n",
            "Step 200, Train Loss: 7.803\n",
            "Step 200, Eval Loss: 5.223\n",
            "Step 300, Train Loss: 4.062\n",
            "Step 300, Eval Loss: 5.186\n",
            "Step 400, Train Loss: 2.614\n",
            "Step 400, Eval Loss: 5.146\n",
            "Step 500, Train Loss: 5.747\n",
            "Step 500, Eval Loss: 5.104\n",
            "Step 600, Train Loss: 24.885\n",
            "Step 600, Eval Loss: 5.065\n",
            "Step 700, Train Loss: 11.340\n",
            "Step 700, Eval Loss: 5.025\n",
            "Step 800, Train Loss: 25.919\n",
            "Step 800, Eval Loss: 4.989\n",
            "Step 900, Train Loss: 7.570\n",
            "Step 900, Eval Loss: 4.948\n",
            "Step 1000, Train Loss: 15.817\n",
            "Step 1000, Eval Loss: 4.906\n"
          ]
        }
      ],
      "source": [
        "training_steps = 1000\n",
        "optimizer = optim.Adam(eb.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in eb.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    eb,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        "    model_type=\"transformer\",\n",
        "    mask=mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>LAST</th>\n",
              "      <th>OPEN</th>\n",
              "      <th>LOW</th>\n",
              "      <th>HIGH</th>\n",
              "      <th>3M IMPLIED VOL</th>\n",
              "      <th>SHORT INTEREST RATIO</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>tweet_sentiment</th>\n",
              "      <th>news_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2006-02-06</td>\n",
              "      <td>126.60</td>\n",
              "      <td>126.44</td>\n",
              "      <td>126.17</td>\n",
              "      <td>126.80</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2006-02-07</td>\n",
              "      <td>125.48</td>\n",
              "      <td>126.30</td>\n",
              "      <td>125.40</td>\n",
              "      <td>126.66</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2006-02-08</td>\n",
              "      <td>126.62</td>\n",
              "      <td>125.88</td>\n",
              "      <td>125.60</td>\n",
              "      <td>126.78</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2006-02-09</td>\n",
              "      <td>126.41</td>\n",
              "      <td>126.85</td>\n",
              "      <td>126.37</td>\n",
              "      <td>127.60</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2006-02-10</td>\n",
              "      <td>126.64</td>\n",
              "      <td>126.42</td>\n",
              "      <td>125.45</td>\n",
              "      <td>127.13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SPY</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         DATE    LAST    OPEN     LOW    HIGH  3M IMPLIED VOL  \\\n",
              "0  2006-02-06  126.60  126.44  126.17  126.80             NaN   \n",
              "1  2006-02-07  125.48  126.30  125.40  126.66             NaN   \n",
              "2  2006-02-08  126.62  125.88  125.60  126.78             NaN   \n",
              "3  2006-02-09  126.41  126.85  126.37  127.60             NaN   \n",
              "4  2006-02-10  126.64  126.42  125.45  127.13             NaN   \n",
              "\n",
              "   SHORT INTEREST RATIO Ticker  tweet_sentiment  news_sentiment  \n",
              "0                   NaN    SPY              NaN             NaN  \n",
              "1                   NaN    SPY              NaN             NaN  \n",
              "2                   NaN    SPY              NaN             NaN  \n",
              "3                   NaN    SPY              NaN             NaN  \n",
              "4                   NaN    SPY              NaN             NaN  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "stocks = pd.read_csv(\"data/stocks.csv\")\n",
        "stocks.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "spy = stocks.loc[\n",
        "    stocks[\"Ticker\"] == \"SPY\", [\"DATE\", \"LAST\", \"OPEN\", \"LOW\", \"HIGH\", \"3M IMPLIED VOL\"]\n",
        "].dropna()\n",
        "spy.reset_index(drop=True, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create y\n",
        "spy[\"price_1d\"] = spy[\"LAST\"].shift(-1)\n",
        "spy[\"price_5d\"] = spy[\"LAST\"].shift(-5)\n",
        "spy[\"price_10d\"] = spy[\"LAST\"].shift(-10)\n",
        "spy[\"price_20d\"] = spy[\"LAST\"].shift(-20)\n",
        "spy[\"open_1d\"] = spy[\"OPEN\"].shift(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [],
      "source": [
        "# divide the data into periods of 251 days\n",
        "n = 251\n",
        "spy[\"period\"] = spy.index // n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\cerva\\AppData\\Local\\Temp\\ipykernel_32772\\274159068.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  spy.groupby(\"period\")\n"
          ]
        }
      ],
      "source": [
        "# for each period, drop the last 20 rows to avoid lookahead bias\n",
        "spy = (\n",
        "    spy.groupby(\"period\")\n",
        "    .apply(\n",
        "        lambda x: x.iloc[:-20], include_groups=True\n",
        "    )  # include groups to later create test query\n",
        "    .reset_index(drop=True)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DATE</th>\n",
              "      <th>LAST</th>\n",
              "      <th>OPEN</th>\n",
              "      <th>LOW</th>\n",
              "      <th>HIGH</th>\n",
              "      <th>3M IMPLIED VOL</th>\n",
              "      <th>price_1d</th>\n",
              "      <th>price_5d</th>\n",
              "      <th>price_10d</th>\n",
              "      <th>price_20d</th>\n",
              "      <th>open_1d</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>period</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-04-15</td>\n",
              "      <td>210.4300</td>\n",
              "      <td>210.05</td>\n",
              "      <td>209.95</td>\n",
              "      <td>211.0400</td>\n",
              "      <td>14.0229</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-04-13</td>\n",
              "      <td>208.0008</td>\n",
              "      <td>207.00</td>\n",
              "      <td>206.84</td>\n",
              "      <td>208.1000</td>\n",
              "      <td>14.9924</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-04-11</td>\n",
              "      <td>235.0600</td>\n",
              "      <td>234.90</td>\n",
              "      <td>233.34</td>\n",
              "      <td>235.1800</td>\n",
              "      <td>11.4890</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-04-11</td>\n",
              "      <td>263.7600</td>\n",
              "      <td>263.47</td>\n",
              "      <td>263.39</td>\n",
              "      <td>265.6400</td>\n",
              "      <td>16.1082</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-04-10</td>\n",
              "      <td>288.2900</td>\n",
              "      <td>287.77</td>\n",
              "      <td>287.31</td>\n",
              "      <td>288.3899</td>\n",
              "      <td>13.0733</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              DATE      LAST    OPEN     LOW      HIGH  3M IMPLIED VOL  \\\n",
              "period                                                                   \n",
              "0       2015-04-15  210.4300  210.05  209.95  211.0400         14.0229   \n",
              "1       2016-04-13  208.0008  207.00  206.84  208.1000         14.9924   \n",
              "2       2017-04-11  235.0600  234.90  233.34  235.1800         11.4890   \n",
              "3       2018-04-11  263.7600  263.47  263.39  265.6400         16.1082   \n",
              "4       2019-04-10  288.2900  287.77  287.31  288.3899         13.0733   \n",
              "\n",
              "        price_1d  price_5d  price_10d  price_20d  open_1d  \n",
              "period                                                     \n",
              "0            0.0       0.0        0.0        0.0      0.0  \n",
              "1            0.0       0.0        0.0        0.0      0.0  \n",
              "2            0.0       0.0        0.0        0.0      0.0  \n",
              "3            0.0       0.0        0.0        0.0      0.0  \n",
              "4            0.0       0.0        0.0        0.0      0.0  "
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now let's fill the last entry of each period with 0.0\n",
        "columns = [\"price_1d\", \"price_5d\", \"price_10d\", \"price_20d\", \"open_1d\"]\n",
        "last_entry = spy.groupby(\"period\").apply(\n",
        "    lambda x: x.last_valid_index(), include_groups=False\n",
        ")\n",
        "spy_targets = np.array(spy.loc[last_entry, columns])\n",
        "spy.loc[last_entry, columns] = 0.0\n",
        "spy.groupby(\"period\").last().head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We're going to treat each period as a context\n",
        "spy = spy.drop(columns=[\"DATE\"])\n",
        "groups = [group.drop(columns=[\"period\"]).values for _, group in spy.groupby(\"period\")]\n",
        "spy_np = np.array(groups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss is 295493.369.\n"
          ]
        }
      ],
      "source": [
        "from scratch_transformer import (\n",
        "    LayerNormalization,\n",
        "    FeedForwardBlock,\n",
        "    ResidualConnection,\n",
        "    EncoderBlock,\n",
        "    MultiHeadAttentionBlock,\n",
        "    Encoder,\n",
        ")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "feature_size = spy_np.shape[2]\n",
        "output_size = 1\n",
        "lr = 1e-4\n",
        "dropout = 0.2\n",
        "mask = None\n",
        "heads = 2\n",
        "layers = 2\n",
        "\n",
        "# convert to tensor\n",
        "e = torch.tensor(spy_np).float()\n",
        "\n",
        "\n",
        "# MLP dimension usually 4 times the d_model\n",
        "# Residual connection already contains layer normalizations\n",
        "\n",
        "# Start with Self Attention\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size, heads=heads, dropout=dropout, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Feed Forward\n",
        "ff = FeedForwardBlock(\n",
        "    d_model=feature_size, d_ff=4 * feature_size, dropout=dropout\n",
        ")  # (batch_size, seq_len, d_model\n",
        "\n",
        "\n",
        "# Create an EncoderBlock\n",
        "eb = EncoderBlock(\n",
        "    self_attention_block=mha,\n",
        "    feed_forward_block=ff,\n",
        "    dropout=dropout,\n",
        ")\n",
        "\n",
        "encoder_blocks = []\n",
        "for _ in range(layers):\n",
        "    encoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "        d_model=feature_size, heads=heads, dropout=dropout\n",
        "    )\n",
        "    encoder_feed_forward_block = FeedForwardBlock(\n",
        "        d_model=feature_size, d_ff=4 * feature_size, dropout=dropout\n",
        "    )\n",
        "    encoder_block = EncoderBlock(\n",
        "        self_attention_block=encoder_self_attention_block,\n",
        "        feed_forward_block=encoder_feed_forward_block,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "    encoder_blocks.append(encoder_block)\n",
        "\n",
        "# Don't worry about Encoder, it's just predefined in scratch_transformer\n",
        "decoder = Encoder(\n",
        "    nn.ModuleList(encoder_blocks),\n",
        ")\n",
        "\n",
        "for p in decoder.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "out = decoder(e, mask)\n",
        "# Compare the output to the targets\n",
        "eval_targets = spy_targets\n",
        "eval_preds = out[:, -1, -5:] * (-1.0)\n",
        "\n",
        "loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "print(f\"Loss is {loss:.3f}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_steps = 1000\n",
        "optimizer = optim.Adam(eb.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "total_params = sum(p.numel() for p in eb.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    eb,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        "    model_type=\"transformer\",\n",
        "    mask=mask,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
