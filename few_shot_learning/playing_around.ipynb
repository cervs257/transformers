{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLOp3W1o5ENS",
        "outputId": "3daf093c-cca0-41b1-f580-373b2db1b2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 74 (delta 29), reused 61 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 8.98 MiB | 26.14 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/transformers/few_shot_learning\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cervs257/transformers\n",
        "%cd transformers/few_shot_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ba9jIIhI49Kr"
      },
      "outputs": [],
      "source": [
        "from scratch_transformer import MultiHeadAttentionBlock\n",
        "from data import create_weights, get_reg_data, get_nonlinear_data\n",
        "import numpy as np\n",
        "\n",
        "feature_size = 10\n",
        "output_size = 1\n",
        "M = 10\n",
        "N = 1000\n",
        "lr = 1e-3\n",
        "\n",
        "# linear attention params override\n",
        "la_params = create_weights(feature_size, output_size, N, lr)\n",
        "\n",
        "# get the data\n",
        "eval_data = get_reg_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JIeEmuz949Ks"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Now we will override the weights of the model to implement those that perform GD in the forward pass\n",
        "def override_weights(model, new_params, w_name):\n",
        "    w_name = \"Transformer_gd/multi_head_attention/\" + w_name\n",
        "    w_numpy = new_params[w_name][\"w\"]\n",
        "    w_tensor = torch.tensor(w_numpy, dtype=model.weight.dtype)\n",
        "    model.weight.data = w_tensor\n",
        "\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha.w_q, la_params, \"query\")\n",
        "override_weights(mha.w_k, la_params, \"key\")\n",
        "override_weights(mha.w_v, la_params, \"value\")\n",
        "override_weights(mha.w_o, la_params, \"linear\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JnmTyGrv49Ks"
      },
      "outputs": [],
      "source": [
        "def compute_loss(preds, targets):\n",
        "    \"\"\"Compute the MSE loss.\"\"\"\n",
        "    return 0.5 * np.sum((targets - preds) ** 2) / targets.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0fnTyjvR49Ks"
      },
      "outputs": [],
      "source": [
        "e_eval = torch.tensor(eval_data[0]).float()\n",
        "\n",
        "# Forward pass\n",
        "out = mha(e_eval, e_eval, e_eval)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_targets = eval_data[1][:, -1]\n",
        "eval_preds = out[:, -1, -1] * (-1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC5hxQxF49Ks",
        "outputId": "5c5d898b-b031-4536-9f8f-f30e37352abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss for M: 10, N: 1000 is 0.153.\n"
          ]
        }
      ],
      "source": [
        "loss = compute_loss(eval_preds.detach().numpy(), eval_targets)\n",
        "print(f\"Loss for M: {M}, N: {N} is {loss:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6xxrFe7R49Kt"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model, optimizer, criterion, eval_data=None, training_steps=1000, linear_data=False\n",
        "):\n",
        "    eval_losses = []\n",
        "    lowest_loss = 1e9\n",
        "\n",
        "    # Move the model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print(f\"Training on {device}.\")\n",
        "\n",
        "    # Get the evaluation data if it is not provided\n",
        "    if eval_data is None:\n",
        "        if linear_data:\n",
        "            eval_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            eval_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "    assert eval_data is not None, \"No evaluation data provided.\"\n",
        "    e_eval = torch.tensor(eval_data[0]).float().to(device)\n",
        "    eval_targets = torch.tensor(eval_data[1][:, -1]).float().to(device)\n",
        "    for step in range(training_steps + 1):\n",
        "        # Generate train data\n",
        "        if linear_data:\n",
        "            train_data = get_reg_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        else:\n",
        "            train_data = get_nonlinear_data(\n",
        "                no_tasks=M, feature_size=feature_size, no_examples=N\n",
        "            )\n",
        "        e_train = torch.tensor(train_data[0]).float().to(device)\n",
        "        targets = torch.tensor(train_data[1][:, -1]).float().to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        out = model(e_train, e_train, e_train)\n",
        "        preds = out[:, -1, -1] * (-1.0)\n",
        "        loss = criterion(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Evaluate\n",
        "        if step % 100 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                ev_preds = model(e_eval, e_eval, e_eval)\n",
        "                ev_preds = ev_preds[:, -1, -1] * (-1.0)\n",
        "                eval_loss = criterion(ev_preds, eval_targets)\n",
        "                eval_losses.append(eval_loss)\n",
        "            model.train()\n",
        "            if eval_loss < lowest_loss:\n",
        "                lowest_loss = eval_loss\n",
        "                if linear_data:\n",
        "                    data_type = \"lin_data\"\n",
        "                else:\n",
        "                    data_type = \"nonlin_data\"\n",
        "                if model.softmax_att:\n",
        "                    att = \"softmax_attn\"\n",
        "                else:\n",
        "                    att = \"linear_attn\"\n",
        "                path = f\"models/{att}-{data_type}.pth\"\n",
        "                torch.save(model.state_dict(), path)\n",
        "            print(f\"Step {step}, Train Loss: {loss.item():.3f}\")\n",
        "            print(f\"Step {step}, Eval Loss: {eval_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEqqs2G49Kt",
        "outputId": "977e0c6f-f9f8-4e35-aff5-039a00562fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 0.811\n",
            "Step 0, Eval Loss: 0.231\n",
            "Step 100, Train Loss: 0.022\n",
            "Step 100, Eval Loss: 0.023\n",
            "Step 200, Train Loss: 0.054\n",
            "Step 200, Eval Loss: 0.019\n",
            "Step 300, Train Loss: 0.016\n",
            "Step 300, Eval Loss: 0.019\n",
            "Step 400, Train Loss: 0.012\n",
            "Step 400, Eval Loss: 0.013\n",
            "Step 500, Train Loss: 0.013\n",
            "Step 500, Eval Loss: 0.017\n",
            "Step 600, Train Loss: 0.019\n",
            "Step 600, Eval Loss: 0.010\n",
            "Step 700, Train Loss: 0.027\n",
            "Step 700, Eval Loss: 0.011\n",
            "Step 800, Train Loss: 0.018\n",
            "Step 800, Eval Loss: 0.012\n",
            "Step 900, Train Loss: 0.005\n",
            "Step 900, Eval Loss: 0.014\n",
            "Step 1000, Train Loss: 0.007\n",
            "Step 1000, Eval Loss: 0.018\n"
          ]
        }
      ],
      "source": [
        "# Now let's explore training the model\n",
        "import torch.optim as optim\n",
        "\n",
        "# Train\n",
        "optimizer = optim.Adam(mha.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "train(\n",
        "    mha,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ity6FBVo49Kt",
        "outputId": "ad566c23-bf13-4eb0-b022-018aabddb0ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss pre override for M: 10, N: 1000 is 1059.016.\n",
            "Loss with GD weights for M: 10, N: 1000 is 0.603.\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "# Let's do the same but with non linear data\n",
        "eval_nl_data = get_nonlinear_data(no_tasks=M, feature_size=feature_size, no_examples=N)\n",
        "e_eval_nl = torch.tensor(eval_nl_data[0]).float()\n",
        "\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=False\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "# Forward pass pre override\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss pre override for M: {M}, N: {N} is {loss_nl:.3f}.\")\n",
        "\n",
        "# Override the weights of the model\n",
        "override_weights(mha_nl.w_q, la_params, \"query\")\n",
        "override_weights(mha_nl.w_k, la_params, \"key\")\n",
        "override_weights(mha_nl.w_v, la_params, \"value\")\n",
        "override_weights(mha_nl.w_o, la_params, \"linear\")\n",
        "\n",
        "# Forward pass\n",
        "out_nl = mha_nl(e_eval_nl, e_eval_nl, e_eval_nl)\n",
        "\n",
        "# Compare the output to the targets\n",
        "eval_nl_targets = eval_nl_data[1][:, -1]\n",
        "eval_nl_preds = out_nl[:, -1, -1] * (-1.0)\n",
        "\n",
        "loss_nl = compute_loss(eval_nl_preds.detach().numpy(), eval_nl_targets)\n",
        "print(f\"Loss with GD weights for M: {M}, N: {N} is {loss_nl:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CSkn2F49Kt",
        "outputId": "86db883d-4d89-41c1-bde0-726f5592028b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 1.222\n",
            "Step 0, Eval Loss: 8.459\n",
            "Step 100, Train Loss: 1.940\n",
            "Step 100, Eval Loss: 2.947\n",
            "Step 200, Train Loss: 1.526\n",
            "Step 200, Eval Loss: 1.124\n",
            "Step 300, Train Loss: 2.347\n",
            "Step 300, Eval Loss: 1.023\n",
            "Step 400, Train Loss: 1.603\n",
            "Step 400, Eval Loss: 1.079\n",
            "Step 500, Train Loss: 1.361\n",
            "Step 500, Eval Loss: 1.208\n",
            "Step 600, Train Loss: 2.761\n",
            "Step 600, Eval Loss: 1.065\n",
            "Step 700, Train Loss: 0.738\n",
            "Step 700, Eval Loss: 1.183\n",
            "Step 800, Train Loss: 1.057\n",
            "Step 800, Eval Loss: 1.669\n",
            "Step 900, Train Loss: 2.104\n",
            "Step 900, Eval Loss: 1.173\n",
            "Step 1000, Train Loss: 1.093\n",
            "Step 1000, Eval Loss: 1.194\n",
            "Step 1100, Train Loss: 0.666\n",
            "Step 1100, Eval Loss: 1.440\n",
            "Step 1200, Train Loss: 1.182\n",
            "Step 1200, Eval Loss: 1.303\n",
            "Step 1300, Train Loss: 1.037\n",
            "Step 1300, Eval Loss: 1.566\n",
            "Step 1400, Train Loss: 1.827\n",
            "Step 1400, Eval Loss: 1.525\n",
            "Step 1500, Train Loss: 0.792\n",
            "Step 1500, Eval Loss: 1.274\n",
            "Step 1600, Train Loss: 1.173\n",
            "Step 1600, Eval Loss: 1.125\n",
            "Step 1700, Train Loss: 1.082\n",
            "Step 1700, Eval Loss: 1.044\n",
            "Step 1800, Train Loss: 0.858\n",
            "Step 1800, Eval Loss: 1.007\n",
            "Step 1900, Train Loss: 0.447\n",
            "Step 1900, Eval Loss: 0.985\n",
            "Step 2000, Train Loss: 1.200\n",
            "Step 2000, Eval Loss: 1.018\n",
            "Step 2100, Train Loss: 1.545\n",
            "Step 2100, Eval Loss: 1.113\n",
            "Step 2200, Train Loss: 1.016\n",
            "Step 2200, Eval Loss: 1.070\n",
            "Step 2300, Train Loss: 0.966\n",
            "Step 2300, Eval Loss: 1.090\n",
            "Step 2400, Train Loss: 0.610\n",
            "Step 2400, Eval Loss: 1.077\n",
            "Step 2500, Train Loss: 0.830\n",
            "Step 2500, Eval Loss: 0.959\n",
            "Step 2600, Train Loss: 0.696\n",
            "Step 2600, Eval Loss: 0.933\n",
            "Step 2700, Train Loss: 0.993\n",
            "Step 2700, Eval Loss: 1.157\n",
            "Step 2800, Train Loss: 0.278\n",
            "Step 2800, Eval Loss: 1.183\n",
            "Step 2900, Train Loss: 0.645\n",
            "Step 2900, Eval Loss: 0.987\n",
            "Step 3000, Train Loss: 1.080\n",
            "Step 3000, Eval Loss: 1.197\n"
          ]
        }
      ],
      "source": [
        "lr = 5e-4\n",
        "optimizer = optim.Adam(mha_nl.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 3000\n",
        "\n",
        "# Now let's explore training the model\n",
        "train(\n",
        "    mha_nl,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=eval_nl_data,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqMN7ZO049Kt",
        "outputId": "c20dadb0-ed18-46cc-af6c-dc74d32f8e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda.\n",
            "Step 0, Train Loss: 0.778\n",
            "Step 0, Eval Loss: 0.747\n",
            "Step 100, Train Loss: 0.854\n",
            "Step 100, Eval Loss: 0.677\n",
            "Step 200, Train Loss: 0.770\n",
            "Step 200, Eval Loss: 0.567\n",
            "Step 300, Train Loss: 0.759\n",
            "Step 300, Eval Loss: 0.455\n",
            "Step 400, Train Loss: 0.794\n",
            "Step 400, Eval Loss: 0.432\n",
            "Step 500, Train Loss: 0.550\n",
            "Step 500, Eval Loss: 0.414\n",
            "Step 600, Train Loss: 0.581\n",
            "Step 600, Eval Loss: 0.449\n",
            "Step 700, Train Loss: 1.445\n",
            "Step 700, Eval Loss: 0.418\n",
            "Step 800, Train Loss: 0.475\n",
            "Step 800, Eval Loss: 0.403\n",
            "Step 900, Train Loss: 0.499\n",
            "Step 900, Eval Loss: 0.399\n",
            "Step 1000, Train Loss: 0.486\n",
            "Step 1000, Eval Loss: 0.427\n"
          ]
        }
      ],
      "source": [
        "# Finally let's use softmax attention\n",
        "# Create a MultiHeadAttentionBlock\n",
        "mha_nl_sa = MultiHeadAttentionBlock(\n",
        "    d_model=feature_size + 1, heads=1, dropout=0.0, softmax_att=True\n",
        ")  # (batch_size, seq_len, d_model)\n",
        "\n",
        "optimizer = optim.Adam(mha_nl_sa.parameters(), lr=lr)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "training_steps = 1000\n",
        "\n",
        "# Training the model\n",
        "train(\n",
        "    mha_nl_sa,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    eval_data=None,\n",
        "    training_steps=training_steps,\n",
        "    linear_data=False,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
